{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e991cb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx      class                                       conversation\n",
       "0    0      협박 대화  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...\n",
       "1    1      협박 대화  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...\n",
       "2    2  기타 괴롭힘 대화  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...\n",
       "3    3      갈취 대화  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...\n",
       "4    4      갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "train_data_path = os.getenv('HOME')+\"/aiffel/dktc/data/train.csv\"\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f3edb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idx             3950\n",
       "class              4\n",
       "conversation    3846\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d7f2f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['협박 대화', '기타 괴롭힘 대화', '갈취 대화', '직장 내 괴롭힘 대화'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b687592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data['conversation'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bebdc8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n"
     ]
    }
   ],
   "source": [
    "start = 0 \n",
    "for i in train_data['conversation'].values:\n",
    "    length = max(start,len(i))\n",
    "    start = len(i)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7519dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>3</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>준하야 넌 대가리가 왜이렇게 크냐?\\n내 머리가 뭐.\\n밥먹으면 대가리만 크냐 너는...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>1</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>내가 지금 너 아들 김길준 데리고 있어. 살리고 싶으면 계좌에 1억만 보내\\n예.?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>나는 씨 같은 사람 보면 참 신기하더라. 어떻게 저렇게 살지.\\n왜 그래. 들리겠어...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>1</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>누구맘대로 여기서 장사하래?\\n이게 무슨일입니까?\\n남의 구역에서 장사하려면 자릿세...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3949</th>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>희정씨\\n네?\\n주말에 시간이 넘쳐나나봐\\n갑자기 왜그러세요?\\n손이 빤짝빤짝 네일...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3950 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      idx        class                                       conversation\n",
       "0       0        협박 대화  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...\n",
       "1       0        협박 대화  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...\n",
       "2       3    기타 괴롭힘 대화  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...\n",
       "3       1        갈취 대화  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...\n",
       "4       1        갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...\n",
       "...   ...          ...                                                ...\n",
       "3945    3    기타 괴롭힘 대화  준하야 넌 대가리가 왜이렇게 크냐?\\n내 머리가 뭐.\\n밥먹으면 대가리만 크냐 너는...\n",
       "3946    1        갈취 대화  내가 지금 너 아들 김길준 데리고 있어. 살리고 싶으면 계좌에 1억만 보내\\n예.?...\n",
       "3947    2  직장 내 괴롭힘 대화  나는 씨 같은 사람 보면 참 신기하더라. 어떻게 저렇게 살지.\\n왜 그래. 들리겠어...\n",
       "3948    1        갈취 대화  누구맘대로 여기서 장사하래?\\n이게 무슨일입니까?\\n남의 구역에서 장사하려면 자릿세...\n",
       "3949    2  직장 내 괴롭힘 대화  희정씨\\n네?\\n주말에 시간이 넘쳐나나봐\\n갑자기 왜그러세요?\\n손이 빤짝빤짝 네일...\n",
       "\n",
       "[3950 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = train_data\n",
    "df.loc[ df['class']=='협박 대화', 'idx'] = 0\n",
    "df.loc[ df['class']=='기타 괴롭힘 대화', 'idx'] = 3\n",
    "df.loc[ df['class']=='갈취 대화', 'idx'] = 1\n",
    "df.loc[ df['class']=='직장 내 괴롭힘 대화', 'idx'] = 2\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d50731f",
   "metadata": {},
   "source": [
    "### DistilBERT\n",
    "-  BERT를 직접 사용하지 않고 BERT의 동작을 모방하는 단순화된 모델(BERT 매개변수의 절반만 가짐)인 DistilBERT를 사용하여 작업\n",
    "- 매개변수가 절반만 있음에도 불구하고 DistilBERT의 성능은 BERT의 성능과 매우 비슷합니다\n",
    "-  BERT는 텍스트를 벡터로 인코딩하는 데 사용\n",
    "- BERT를 기반으로 구축한 분류 모델은 ML 알고리즘을 사용하여 이러한 벡터를 분류하는 것으로 구성\n",
    "- 전처리 단계로 텍스트에 BERT를 적용한 다음 이러한 벡터를 분류하는 ML 모델을 구축합니다.\n",
    "- BERT로 시작하는 모델을 구축하고(모델을 훈련할 때 매개변수를 고정함) BERT 위에 이러한 벡터를 분류하는 레이어를 구축합니다.\n",
    "\n",
    "\n",
    "- LLM(대형 언어 모델)의 경우 데이터를 수동으로 더 이상 처리할 필요가 없습니다. 토크나이저, 즉 텍스트를 \"숫자\"로 인코딩하는 레이어가 모델과 함께 제공되기 때문입니다.\n",
    "\n",
    "\n",
    "- 모델을 만들어 보겠습니다. 이를 위해서는 먼저 변환기 라이브러리에서 BERT 레이어를 가져와야 합니다. 훈련 중에 매개변수가 훈련되지 않도록 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "18b0b676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 거면 혼자 죽지 우리까지 사건에 휘말리게 해? 진짜 죽여버리고 싶게.\\n 정말 잘못했습니다.\\n 너가 선택해. 너가 죽을래 네 가족을 죽여줄까.\\n 죄송합니다. 정말 잘못했습니다.\\n 너에게는 선택권이 없어. 선택 못한다면 너와 네 가족까지 모조리 죽여버릴거야.\\n 선택 못하겠습니다. 한번만 도와주세요.\\n 그냥 다 죽여버려야겠군. 이의 없지?\\n 제발 도와주세요.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X= df['conversation']\n",
    "y = df['idx']\n",
    "\n",
    "# Show the data\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c485cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distilBERT tokenizer\n",
    "import transformers\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5511b772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data:  (2765, 225)\n",
      "Shape of test data:  (1185, 225)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# DistilBertTokenizer로 모든 문장 토큰화\n",
    "X_tf = [tokenizer(text, padding='max_length', max_length = 225, truncation=True)['input_ids'] for text in X]\n",
    "X_tf = np.array(X_tf, dtype='int32')\n",
    "# Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tf_train, X_tf_test, y_tf_train, y_tf_test = train_test_split(X_tf, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print('Shape of training data: ',X_tf_train.shape)\n",
    "print('Shape of test data: ',X_tf_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ae77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a09998f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_projector', 'vocab_transform', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get BERT layer\n",
    "import transformers\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "config = transformers.DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
    "dbert_tf = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bfc7321b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object type:  <class 'transformers.modeling_tf_outputs.TFBaseModelOutput'>\n",
      "Output format (shape):  (5, 225, 768)\n",
      "Output used as input for the classifier (shape):  (5, 768)\n"
     ]
    }
   ],
   "source": [
    "# Let's create a sample of size 5 from the training data\n",
    "sample = X_tf_train[0:5]\n",
    "print('Object type: ', type(dbert_tf(sample)))\n",
    "print('Output format (shape): ',dbert_tf(sample)[0].shape)\n",
    "print('Output used as input for the classifier (shape): ', dbert_tf(sample)[0][:,0,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d12845e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ClassificationModelTF\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_token (InputLayer)     [(None, 225)]             0         \n",
      "_________________________________________________________________\n",
      "tf_distil_bert_model_4 (TFDi TFBaseModelOutput(last_hi 66362880  \n",
      "_________________________________________________________________\n",
      "tf.__operators__.getitem_2 ( (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 64)                49216     \n",
      "_________________________________________________________________\n",
      "classification (Dense)       (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 66,412,356\n",
      "Trainable params: 49,476\n",
      "Non-trainable params: 66,362,880\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models, layers, metrics\n",
    "\n",
    "input_ids_in = layers.Input(shape=(225,), name='input_token', dtype='int32')\n",
    "\n",
    "x = dbert_tf(input_ids=input_ids_in)[0][:,0,:]\n",
    "x = layers.Dropout(0.5, name='dropout')(x)\n",
    "x = layers.Dense(64, activation='relu', name='dense2')(x)\n",
    "x = layers.Dense(4, activation='softmax', name='classification')(x)\n",
    "\n",
    "model_tf = models.Model(inputs=input_ids_in, outputs = x, name='ClassificationModelTF')\n",
    "\n",
    "model_tf.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=[metrics.SparseCategoricalAccuracy()])\n",
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c7d7733a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "44/44 [==============================] - 37s 725ms/step - loss: 1.4194 - sparse_categorical_accuracy: 0.2720 - val_loss: 1.3829 - val_sparse_categorical_accuracy: 0.3105\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 30s 696ms/step - loss: 1.3946 - sparse_categorical_accuracy: 0.2749 - val_loss: 1.3710 - val_sparse_categorical_accuracy: 0.3072\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 30s 682ms/step - loss: 1.3867 - sparse_categorical_accuracy: 0.2626 - val_loss: 1.3739 - val_sparse_categorical_accuracy: 0.2920\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 30s 691ms/step - loss: 1.3797 - sparse_categorical_accuracy: 0.2774 - val_loss: 1.3713 - val_sparse_categorical_accuracy: 0.2827\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 30s 691ms/step - loss: 1.3739 - sparse_categorical_accuracy: 0.2922 - val_loss: 1.3681 - val_sparse_categorical_accuracy: 0.3055\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 30s 688ms/step - loss: 1.3710 - sparse_categorical_accuracy: 0.3005 - val_loss: 1.3692 - val_sparse_categorical_accuracy: 0.2878\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 30s 690ms/step - loss: 1.3691 - sparse_categorical_accuracy: 0.2926 - val_loss: 1.3731 - val_sparse_categorical_accuracy: 0.2886\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 30s 691ms/step - loss: 1.3719 - sparse_categorical_accuracy: 0.2929 - val_loss: 1.3665 - val_sparse_categorical_accuracy: 0.2945\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 30s 690ms/step - loss: 1.3743 - sparse_categorical_accuracy: 0.2984 - val_loss: 1.3665 - val_sparse_categorical_accuracy: 0.3030\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.3670 - sparse_categorical_accuracy: 0.2995"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_837/2911195676.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tf_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1213\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1215\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1216\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# Train the model\n",
    "start_time = datetime.now()\n",
    "history = model_tf.fit(X_tf_train, y_tf_train, batch_size=64, shuffle=True, epochs=100, validation_data=(X_tf_test, y_tf_test))\n",
    "end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96a2b17",
   "metadata": {},
   "outputs": [],
   "source": [
    ". fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5)) \n",
    "ax[0].set(title='Loss') \n",
    "ax[0].plot( History.history['loss'], label='Training') \n",
    "ax[0].plot(history.history['val_loss'], label='Validation') \n",
    "ax[0].legend(loc=\"오른쪽 위\" ) \n",
    "\n",
    "ax[1].set(title='정확도') \n",
    "ax[1].plot(history.history['sparse_categorical_accuracy'], label='Training') \n",
    "ax[1].plot(history.history['val_sparse_categorical_accuracy' ], label='검증') \n",
    "ax[1].legend(loc=\"오른쪽 아래\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e83a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy_tf = History.history['val_sparse_categorical_accuracy'][-1] \n",
    "print('정확도 훈련 데이터: {:.1%}'.format(history.history['sparse_categorical_accuracy'][-1])) \n",
    "print('정확도 테스트 데이터: {:.1%}'.format(history.history['val_sparse_categorical_accuracy'][-1])) \n",
    "print('훈련 시간: {:.1f}s(또는 {:.1f}분)'.format (training_time_tf, training_time_tf/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4360e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tf.save('model_tf.h5', save_format='h5')\n",
    "model_tf2 = models.load_model('model_tf.h5', custom_objects={'TFDistilBertModel': dbert_tf})\n",
    "model_tf2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd62898",
   "metadata": {},
   "outputs": [],
   "source": [
    "## V. Sanh et al (2019), “DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter” (https://arxiv.org/abs/1910.01108)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c96fa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9f93b",
   "metadata": {},
   "source": [
    "---\n",
    "## Hugging face Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dd52a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stock ml libraries\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b1e64157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting up the device for GPU usage\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "83920a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "43b2f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df= df[['conversation','idx']].rename(columns={'idx':'labels','conversation':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7324e1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels\n",
       "0  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...       0\n",
       "1  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...       0\n",
       "2  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...       3\n",
       "3  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...       1\n",
       "4  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...       1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c7fdd359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>준하야 넌 대가리가 왜이렇게 크냐?\\n내 머리가 뭐.\\n밥먹으면 대가리만 크냐 너는...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>내가 지금 너 아들 김길준 데리고 있어. 살리고 싶으면 계좌에 1억만 보내\\n예.?...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>나는 씨 같은 사람 보면 참 신기하더라. 어떻게 저렇게 살지.\\n왜 그래. 들리겠어...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>누구맘대로 여기서 장사하래?\\n이게 무슨일입니까?\\n남의 구역에서 장사하려면 자릿세...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3949</th>\n",
       "      <td>희정씨\\n네?\\n주말에 시간이 넘쳐나나봐\\n갑자기 왜그러세요?\\n손이 빤짝빤짝 네일...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3950 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text        labels\n",
       "0     지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...  [1, 0, 0, 0]\n",
       "1     길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...  [1, 0, 0, 0]\n",
       "2     너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...  [0, 0, 0, 1]\n",
       "3     어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...  [0, 1, 0, 0]\n",
       "4     저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...  [0, 1, 0, 0]\n",
       "...                                                 ...           ...\n",
       "3945  준하야 넌 대가리가 왜이렇게 크냐?\\n내 머리가 뭐.\\n밥먹으면 대가리만 크냐 너는...  [0, 0, 0, 1]\n",
       "3946  내가 지금 너 아들 김길준 데리고 있어. 살리고 싶으면 계좌에 1억만 보내\\n예.?...  [0, 1, 0, 0]\n",
       "3947  나는 씨 같은 사람 보면 참 신기하더라. 어떻게 저렇게 살지.\\n왜 그래. 들리겠어...  [0, 0, 1, 0]\n",
       "3948  누구맘대로 여기서 장사하래?\\n이게 무슨일입니까?\\n남의 구역에서 장사하려면 자릿세...  [0, 1, 0, 0]\n",
       "3949  희정씨\\n네?\\n주말에 시간이 넘쳐나나봐\\n갑자기 왜그러세요?\\n손이 빤짝빤짝 네일...  [0, 0, 1, 0]\n",
       "\n",
       "[3950 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# label을 원핫 인코딩\n",
    "mlb = MultiLabelBinarizer()\n",
    "new_df['labels'] = mlb.fit_transform(new_df['labels'].apply(lambda x: [x])).tolist()\n",
    "\n",
    "# 'label' 컬럼 제거\n",
    "#new_df.drop('labels', axis=1, inplace=True)\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ba9de165",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 200\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "496d1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.data.text[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        #token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            #'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.labels[index], dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "124f3162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (3950, 3)\n",
      "TRAIN Dataset: (3160, 2)\n",
      "TEST Dataset: (790, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "train_size = 0.8\n",
    "train_dataset=new_df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=new_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = MultiLabelDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = MultiLabelDataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9e864e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f2ae1e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "#768\n",
    "class DistillBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistillBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 128)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.classifier = torch.nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "\n",
    "model = DistillBERTClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8fdc1e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9114c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c18dbe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    \n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids']\n",
    "        mask = data['mask']\n",
    "        targets = data['targets']\n",
    "        #token_type_ids = data['token_type_ids']\n",
    "\n",
    "        outputs = model(ids, mask)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 100 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 100 steps: {accu_step}%\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d6269db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 100 steps: 1.4122552871704102\n",
      "Training Accuracy per 100 steps: 0.0%\n",
      "Training Loss per 100 steps: 1.397143816003705\n",
      "Training Accuracy per 100 steps: 23.26732673267327%\n",
      "Training Loss per 100 steps: 1.398432371035144\n",
      "Training Accuracy per 100 steps: 23.134328358208954%\n",
      "Training Loss per 100 steps: 1.3998022891358284\n",
      "Training Accuracy per 100 steps: 22.591362126245848%\n",
      "Training Loss per 100 steps: 1.3972342531579987\n",
      "Training Accuracy per 100 steps: 23.19201995012469%\n",
      "Training Loss per 100 steps: 1.3941161646814404\n",
      "Training Accuracy per 100 steps: 23.75249500998004%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_837/361968077.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_837/392048658.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;31m# # When using GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59a1026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(testing_loader):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aa5082",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, targets = validation(testing_loader)\n",
    "\n",
    "final_outputs = np.array(outputs) >=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc7aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_hamming_loss = metrics.hamming_loss(targets, final_outputs)\n",
    "val_hamming_score = hamming_score(np.array(targets), np.array(final_outputs))\n",
    "\n",
    "print(f\"Hamming Score = {val_hamming_score}\")\n",
    "print(f\"Hamming Loss = {val_hamming_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20db81f6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b073b",
   "metadata": {},
   "source": [
    "## ALBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "355f846f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>3</td>\n",
       "      <td>준하야 넌 대가리가 왜이렇게 크냐?\\n내 머리가 뭐.\\n밥먹으면 대가리만 크냐 너는...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>1</td>\n",
       "      <td>내가 지금 너 아들 김길준 데리고 있어. 살리고 싶으면 계좌에 1억만 보내\\n예.?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>2</td>\n",
       "      <td>나는 씨 같은 사람 보면 참 신기하더라. 어떻게 저렇게 살지.\\n왜 그래. 들리겠어...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>1</td>\n",
       "      <td>누구맘대로 여기서 장사하래?\\n이게 무슨일입니까?\\n남의 구역에서 장사하려면 자릿세...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3949</th>\n",
       "      <td>2</td>\n",
       "      <td>희정씨\\n네?\\n주말에 시간이 넘쳐나나봐\\n갑자기 왜그러세요?\\n손이 빤짝빤짝 네일...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3950 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0         0  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...\n",
       "1         0  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...\n",
       "2         3  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...\n",
       "3         1  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...\n",
       "4         1  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...\n",
       "...     ...                                                ...\n",
       "3945      3  준하야 넌 대가리가 왜이렇게 크냐?\\n내 머리가 뭐.\\n밥먹으면 대가리만 크냐 너는...\n",
       "3946      1  내가 지금 너 아들 김길준 데리고 있어. 살리고 싶으면 계좌에 1억만 보내\\n예.?...\n",
       "3947      2  나는 씨 같은 사람 보면 참 신기하더라. 어떻게 저렇게 살지.\\n왜 그래. 들리겠어...\n",
       "3948      1  누구맘대로 여기서 장사하래?\\n이게 무슨일입니까?\\n남의 구역에서 장사하려면 자릿세...\n",
       "3949      2  희정씨\\n네?\\n주말에 시간이 넘쳐나나봐\\n갑자기 왜그러세요?\\n손이 빤짝빤짝 네일...\n",
       "\n",
       "[3950 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_al = df.rename(columns={'idx':'label','conversation':'text'}).drop(['class'],axis = 1)\n",
    "df_al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "376213e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 추가\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification, AdamW\n",
    "\n",
    "\n",
    "# 훈련 데이터셋과 테스트 데이터셋으로 분할\n",
    "train_df, test_df = train_test_split(df_al, test_size=0.2, random_state=42)\n",
    "\n",
    "# Albert 토크나이저 로드\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "# 사용자 정의 데이터셋 클래스\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=64):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataframe.iloc[idx]['text']\n",
    "        label = self.dataframe.iloc[idx]['label']\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "train_dataset = CustomDataset(train_df, tokenizer)\n",
    "test_dataset = CustomDataset(test_df, tokenizer)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Albert 모델 로드\n",
    "model = AlbertForSequenceClassification.from_pretrained(\"albert-base-v2\", num_labels=df_al['label'].nunique())\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2fbe4d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10, Loss: 1.4341071724891663, Accuracy: 0.1875\n",
      "Epoch 1, Batch 20, Loss: 1.419752335548401, Accuracy: 0.225\n",
      "Epoch 1, Batch 30, Loss: 1.399054769674937, Accuracy: 0.2708333333333333\n",
      "Epoch 1, Batch 40, Loss: 1.4040329337120057, Accuracy: 0.26875\n",
      "Epoch 1, Batch 50, Loss: 1.4006766295433044, Accuracy: 0.2675\n",
      "Epoch 1, Batch 60, Loss: 1.4012683471043905, Accuracy: 0.2625\n",
      "Epoch 1, Batch 70, Loss: 1.4047328574316842, Accuracy: 0.2517857142857143\n",
      "Epoch 1, Batch 80, Loss: 1.4038457974791527, Accuracy: 0.25625\n",
      "Epoch 1, Batch 90, Loss: 1.4029256754451327, Accuracy: 0.2625\n",
      "Epoch 1, Batch 100, Loss: 1.4028579449653626, Accuracy: 0.2625\n",
      "Epoch 1, Batch 110, Loss: 1.4013782772150907, Accuracy: 0.26477272727272727\n",
      "Epoch 1, Batch 120, Loss: 1.4033663292725882, Accuracy: 0.26145833333333335\n",
      "Epoch 1, Batch 130, Loss: 1.40383108487496, Accuracy: 0.25961538461538464\n",
      "Epoch 1, Batch 140, Loss: 1.4064994479928699, Accuracy: 0.2544642857142857\n",
      "Epoch 1, Batch 150, Loss: 1.4059185194969177, Accuracy: 0.25666666666666665\n",
      "Epoch 1, Batch 160, Loss: 1.404272112250328, Accuracy: 0.26015625\n",
      "Epoch 1, Batch 170, Loss: 1.403839129560134, Accuracy: 0.2595588235294118\n",
      "Epoch 1, Batch 180, Loss: 1.4049610402848987, Accuracy: 0.26180555555555557\n",
      "Epoch 1, Batch 190, Loss: 1.4039022897419176, Accuracy: 0.2611842105263158\n",
      "Epoch 1, Batch 200, Loss: 1.4033859467506409, Accuracy: 0.259375\n",
      "Epoch 1, Batch 210, Loss: 1.4029924914950416, Accuracy: 0.25892857142857145\n",
      "Epoch 1, Batch 220, Loss: 1.4029617531733078, Accuracy: 0.2585227272727273\n",
      "Epoch 1, Batch 230, Loss: 1.401452156253483, Accuracy: 0.2619565217391304\n",
      "Epoch 1, Batch 240, Loss: 1.4019407565395037, Accuracy: 0.26197916666666665\n",
      "Epoch 1, Batch 250, Loss: 1.4026576023101807, Accuracy: 0.26\n",
      "Epoch 1, Batch 260, Loss: 1.4026253471007715, Accuracy: 0.2567307692307692\n",
      "Epoch 1, Batch 270, Loss: 1.4015549849580835, Accuracy: 0.2587962962962963\n",
      "Epoch 1, Batch 280, Loss: 1.402801617554256, Accuracy: 0.259375\n",
      "Epoch 1, Batch 290, Loss: 1.403460640742861, Accuracy: 0.25948275862068965\n",
      "Epoch 1, Batch 300, Loss: 1.4045260874430339, Accuracy: 0.2575\n",
      "Epoch 1, Batch 310, Loss: 1.4051095208814066, Accuracy: 0.2560483870967742\n",
      "Epoch 1, Batch 320, Loss: 1.4055725641548633, Accuracy: 0.255859375\n",
      "Epoch 1, Batch 330, Loss: 1.4050425236875361, Accuracy: 0.2556818181818182\n",
      "Epoch 1, Batch 340, Loss: 1.4056522600791033, Accuracy: 0.2540441176470588\n",
      "Epoch 1, Batch 350, Loss: 1.4047297784260342, Accuracy: 0.255\n",
      "Epoch 1, Batch 360, Loss: 1.4038223971923192, Accuracy: 0.25729166666666664\n",
      "Epoch 1, Batch 370, Loss: 1.4044247359842867, Accuracy: 0.2577702702702703\n",
      "Epoch 1, Batch 380, Loss: 1.4047864929625862, Accuracy: 0.2575657894736842\n",
      "Epoch 1, Batch 390, Loss: 1.404340346959921, Accuracy: 0.2567307692307692\n",
      "Epoch 1, Training Loss: 1.404338746131221, Accuracy: 0.2579113924050633\n",
      "Epoch 1, Accuracy: 0.2645569620253165\n",
      "Epoch 2, Batch 10, Loss: 1.3906335830688477, Accuracy: 0.325\n",
      "Epoch 2, Batch 20, Loss: 1.3947683274745941, Accuracy: 0.28125\n",
      "Epoch 2, Batch 30, Loss: 1.3997087836265565, Accuracy: 0.2708333333333333\n",
      "Epoch 2, Batch 40, Loss: 1.3927730649709702, Accuracy: 0.275\n",
      "Epoch 2, Batch 50, Loss: 1.3956128525733948, Accuracy: 0.2875\n",
      "Epoch 2, Batch 60, Loss: 1.398983081181844, Accuracy: 0.2875\n",
      "Epoch 2, Batch 70, Loss: 1.4007451074463981, Accuracy: 0.28035714285714286\n",
      "Epoch 2, Batch 80, Loss: 1.3992904782295228, Accuracy: 0.28125\n",
      "Epoch 2, Batch 90, Loss: 1.4000916785664028, Accuracy: 0.28055555555555556\n",
      "Epoch 2, Batch 100, Loss: 1.4001723611354828, Accuracy: 0.27625\n",
      "Epoch 2, Batch 110, Loss: 1.4038836847652088, Accuracy: 0.2761363636363636\n",
      "Epoch 2, Batch 120, Loss: 1.4029277920722962, Accuracy: 0.2760416666666667\n",
      "Epoch 2, Batch 130, Loss: 1.4045562945879422, Accuracy: 0.2721153846153846\n",
      "Epoch 2, Batch 140, Loss: 1.404324037688119, Accuracy: 0.26964285714285713\n",
      "Epoch 2, Batch 150, Loss: 1.4033360719680785, Accuracy: 0.2658333333333333\n",
      "Epoch 2, Batch 160, Loss: 1.4022801510989666, Accuracy: 0.26640625\n",
      "Epoch 2, Batch 170, Loss: 1.4035106588812436, Accuracy: 0.2647058823529412\n",
      "Epoch 2, Batch 180, Loss: 1.4022161715560488, Accuracy: 0.26458333333333334\n",
      "Epoch 2, Batch 190, Loss: 1.4045151478365847, Accuracy: 0.2565789473684211\n",
      "Epoch 2, Batch 200, Loss: 1.4024937707185745, Accuracy: 0.258125\n",
      "Epoch 2, Batch 210, Loss: 1.401904513154711, Accuracy: 0.26071428571428573\n",
      "Epoch 2, Batch 220, Loss: 1.4021570482037284, Accuracy: 0.26022727272727275\n",
      "Epoch 2, Batch 230, Loss: 1.4031598578328672, Accuracy: 0.25869565217391305\n",
      "Epoch 2, Batch 240, Loss: 1.403971905509631, Accuracy: 0.25625\n",
      "Epoch 2, Batch 250, Loss: 1.4019904279708861, Accuracy: 0.259\n",
      "Epoch 2, Batch 260, Loss: 1.4035235904730283, Accuracy: 0.2552884615384615\n",
      "Epoch 2, Batch 270, Loss: 1.4048018919097052, Accuracy: 0.2537037037037037\n",
      "Epoch 2, Batch 280, Loss: 1.4040333198649542, Accuracy: 0.25223214285714285\n",
      "Epoch 2, Batch 290, Loss: 1.4040334335688887, Accuracy: 0.2495689655172414\n",
      "Epoch 2, Batch 300, Loss: 1.403829384247462, Accuracy: 0.2504166666666667\n",
      "Epoch 2, Batch 310, Loss: 1.4047556407989994, Accuracy: 0.24879032258064515\n",
      "Epoch 2, Batch 320, Loss: 1.4037981782108546, Accuracy: 0.250390625\n",
      "Epoch 2, Batch 330, Loss: 1.403365185766509, Accuracy: 0.2515151515151515\n",
      "Epoch 2, Batch 340, Loss: 1.4035238311571234, Accuracy: 0.25183823529411764\n",
      "Epoch 2, Batch 350, Loss: 1.4027675434521267, Accuracy: 0.25142857142857145\n",
      "Epoch 2, Batch 360, Loss: 1.403586557838652, Accuracy: 0.25104166666666666\n",
      "Epoch 2, Batch 370, Loss: 1.4041621024544175, Accuracy: 0.2516891891891892\n",
      "Epoch 2, Batch 380, Loss: 1.4041118734761289, Accuracy: 0.25032894736842104\n",
      "Epoch 2, Batch 390, Loss: 1.4040017610941178, Accuracy: 0.25\n",
      "Epoch 2, Training Loss: 1.4040416032453127, Accuracy: 0.2509493670886076\n",
      "Epoch 2, Accuracy: 0.2645569620253165\n",
      "Epoch 3, Batch 10, Loss: 1.4102887153625487, Accuracy: 0.25\n",
      "Epoch 3, Batch 20, Loss: 1.4000277996063233, Accuracy: 0.275\n",
      "Epoch 3, Batch 30, Loss: 1.3941364765167237, Accuracy: 0.2708333333333333\n",
      "Epoch 3, Batch 40, Loss: 1.3925650000572205, Accuracy: 0.26875\n",
      "Epoch 3, Batch 50, Loss: 1.3999807810783387, Accuracy: 0.265\n",
      "Epoch 3, Batch 60, Loss: 1.3999119699001312, Accuracy: 0.2625\n",
      "Epoch 3, Batch 70, Loss: 1.3944652846881322, Accuracy: 0.275\n",
      "Epoch 3, Batch 80, Loss: 1.3959449708461762, Accuracy: 0.275\n",
      "Epoch 3, Batch 90, Loss: 1.4011097153027852, Accuracy: 0.2791666666666667\n",
      "Epoch 3, Batch 100, Loss: 1.4011968195438385, Accuracy: 0.27875\n",
      "Epoch 3, Batch 110, Loss: 1.4047473560680042, Accuracy: 0.275\n",
      "Epoch 3, Batch 120, Loss: 1.4052764703830083, Accuracy: 0.26666666666666666\n",
      "Epoch 3, Batch 130, Loss: 1.4064640558682955, Accuracy: 0.2644230769230769\n",
      "Epoch 3, Batch 140, Loss: 1.4090746283531188, Accuracy: 0.25625\n",
      "Epoch 3, Batch 150, Loss: 1.4101652510960896, Accuracy: 0.255\n",
      "Epoch 3, Batch 160, Loss: 1.4085018172860146, Accuracy: 0.25625\n",
      "Epoch 3, Batch 170, Loss: 1.4081887834212359, Accuracy: 0.25735294117647056\n",
      "Epoch 3, Batch 180, Loss: 1.4073299719227685, Accuracy: 0.25625\n",
      "Epoch 3, Batch 190, Loss: 1.4040190426926864, Accuracy: 0.2625\n",
      "Epoch 3, Batch 200, Loss: 1.4078792864084244, Accuracy: 0.255625\n",
      "Epoch 3, Batch 210, Loss: 1.408000525974092, Accuracy: 0.25357142857142856\n",
      "Epoch 3, Batch 220, Loss: 1.4082640907981179, Accuracy: 0.25227272727272726\n",
      "Epoch 3, Batch 230, Loss: 1.4077411361362624, Accuracy: 0.25380434782608696\n",
      "Epoch 3, Batch 240, Loss: 1.4060874591271082, Accuracy: 0.2578125\n",
      "Epoch 3, Batch 250, Loss: 1.4065598411560059, Accuracy: 0.256\n",
      "Epoch 3, Batch 260, Loss: 1.4058921694755555, Accuracy: 0.25625\n",
      "Epoch 3, Batch 270, Loss: 1.4062580987259194, Accuracy: 0.25601851851851853\n",
      "Epoch 3, Batch 280, Loss: 1.4059552950518472, Accuracy: 0.2584821428571429\n",
      "Epoch 3, Batch 290, Loss: 1.406077229976654, Accuracy: 0.25862068965517243\n",
      "Epoch 3, Batch 300, Loss: 1.4064617156982422, Accuracy: 0.25583333333333336\n",
      "Epoch 3, Batch 310, Loss: 1.4060608833066879, Accuracy: 0.255241935483871\n",
      "Epoch 3, Batch 320, Loss: 1.406270820274949, Accuracy: 0.2546875\n",
      "Epoch 3, Batch 330, Loss: 1.4064402092586865, Accuracy: 0.2545454545454545\n",
      "Epoch 3, Batch 340, Loss: 1.4059659999959608, Accuracy: 0.2555147058823529\n",
      "Epoch 3, Batch 350, Loss: 1.4072785033498492, Accuracy: 0.2532142857142857\n",
      "Epoch 3, Batch 360, Loss: 1.4071023795339797, Accuracy: 0.253125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 370, Loss: 1.4069146062876727, Accuracy: 0.25304054054054054\n",
      "Epoch 3, Batch 380, Loss: 1.4061209490424709, Accuracy: 0.2536184210526316\n",
      "Epoch 3, Batch 390, Loss: 1.404546693349496, Accuracy: 0.25544871794871793\n",
      "Epoch 3, Training Loss: 1.4048967599868774, Accuracy: 0.25506329113924053\n",
      "Epoch 3, Accuracy: 0.2645569620253165\n",
      "Epoch 4, Batch 10, Loss: 1.4418151736259461, Accuracy: 0.15\n",
      "Epoch 4, Batch 20, Loss: 1.416103345155716, Accuracy: 0.18125\n",
      "Epoch 4, Batch 30, Loss: 1.4116509477297465, Accuracy: 0.20833333333333334\n",
      "Epoch 4, Batch 40, Loss: 1.4107915550470351, Accuracy: 0.215625\n",
      "Epoch 4, Batch 50, Loss: 1.4099556994438172, Accuracy: 0.2325\n",
      "Epoch 4, Batch 60, Loss: 1.4100184659163157, Accuracy: 0.23541666666666666\n",
      "Epoch 4, Batch 70, Loss: 1.4068057349749974, Accuracy: 0.24107142857142858\n",
      "Epoch 4, Batch 80, Loss: 1.4062302604317665, Accuracy: 0.2390625\n",
      "Epoch 4, Batch 90, Loss: 1.4034376819928487, Accuracy: 0.24027777777777778\n",
      "Epoch 4, Batch 100, Loss: 1.3998945260047913, Accuracy: 0.245\n",
      "Epoch 4, Batch 110, Loss: 1.40046372088519, Accuracy: 0.24545454545454545\n",
      "Epoch 4, Batch 120, Loss: 1.4012726108233133, Accuracy: 0.24166666666666667\n",
      "Epoch 4, Batch 130, Loss: 1.401773019020374, Accuracy: 0.2403846153846154\n",
      "Epoch 4, Batch 140, Loss: 1.4027776539325714, Accuracy: 0.2392857142857143\n",
      "Epoch 4, Batch 150, Loss: 1.4025944217046102, Accuracy: 0.23833333333333334\n",
      "Epoch 4, Batch 160, Loss: 1.4017790205776692, Accuracy: 0.24375\n",
      "Epoch 4, Batch 170, Loss: 1.4019671650493846, Accuracy: 0.24411764705882352\n",
      "Epoch 4, Batch 180, Loss: 1.4001909527513716, Accuracy: 0.2465277777777778\n",
      "Epoch 4, Batch 190, Loss: 1.39837566614151, Accuracy: 0.25\n",
      "Epoch 4, Batch 200, Loss: 1.3993053698539735, Accuracy: 0.250625\n",
      "Epoch 4, Batch 210, Loss: 1.3993021283830915, Accuracy: 0.2488095238095238\n",
      "Epoch 4, Batch 220, Loss: 1.4005226389928298, Accuracy: 0.24829545454545454\n",
      "Epoch 4, Batch 230, Loss: 1.399787956217061, Accuracy: 0.24782608695652175\n",
      "Epoch 4, Batch 240, Loss: 1.3993886182705562, Accuracy: 0.25\n",
      "Epoch 4, Batch 250, Loss: 1.3990817041397094, Accuracy: 0.2505\n",
      "Epoch 4, Batch 260, Loss: 1.3999934563269982, Accuracy: 0.2485576923076923\n",
      "Epoch 4, Batch 270, Loss: 1.3995405007291724, Accuracy: 0.24861111111111112\n",
      "Epoch 4, Batch 280, Loss: 1.400908790741648, Accuracy: 0.25\n",
      "Epoch 4, Batch 290, Loss: 1.4026677846908568, Accuracy: 0.24870689655172415\n",
      "Epoch 4, Batch 300, Loss: 1.4027601166566213, Accuracy: 0.24958333333333332\n",
      "Epoch 4, Batch 310, Loss: 1.4027821440850534, Accuracy: 0.24879032258064515\n",
      "Epoch 4, Batch 320, Loss: 1.4026063457131386, Accuracy: 0.247265625\n",
      "Epoch 4, Batch 330, Loss: 1.4040755820996833, Accuracy: 0.2446969696969697\n",
      "Epoch 4, Batch 340, Loss: 1.40390407092431, Accuracy: 0.2452205882352941\n",
      "Epoch 4, Batch 350, Loss: 1.4041353109904697, Accuracy: 0.24785714285714286\n",
      "Epoch 4, Batch 360, Loss: 1.4051145517163806, Accuracy: 0.24756944444444445\n",
      "Epoch 4, Batch 370, Loss: 1.4049085571959212, Accuracy: 0.24797297297297297\n",
      "Epoch 4, Batch 380, Loss: 1.404752763635234, Accuracy: 0.24835526315789475\n",
      "Epoch 4, Batch 390, Loss: 1.4048285261178628, Accuracy: 0.24711538461538463\n",
      "Epoch 4, Training Loss: 1.404690276218366, Accuracy: 0.2468354430379747\n",
      "Epoch 4, Accuracy: 0.26962025316455696\n",
      "Epoch 5, Batch 10, Loss: 1.4302789449691773, Accuracy: 0.175\n",
      "Epoch 5, Batch 20, Loss: 1.4188703298568726, Accuracy: 0.20625\n",
      "Epoch 5, Batch 30, Loss: 1.4163930733998618, Accuracy: 0.20833333333333334\n",
      "Epoch 5, Batch 40, Loss: 1.4098629653453827, Accuracy: 0.21875\n",
      "Epoch 5, Batch 50, Loss: 1.4029356527328491, Accuracy: 0.2425\n",
      "Epoch 5, Batch 60, Loss: 1.4031909386316934, Accuracy: 0.2375\n",
      "Epoch 5, Batch 70, Loss: 1.4043186153684344, Accuracy: 0.24464285714285713\n",
      "Epoch 5, Batch 80, Loss: 1.4088599249720573, Accuracy: 0.2390625\n",
      "Epoch 5, Batch 90, Loss: 1.4078822546535068, Accuracy: 0.23333333333333334\n",
      "Epoch 5, Batch 100, Loss: 1.4062565851211548, Accuracy: 0.23875\n",
      "Epoch 5, Batch 110, Loss: 1.4056060845201666, Accuracy: 0.2340909090909091\n",
      "Epoch 5, Batch 120, Loss: 1.4054084757963816, Accuracy: 0.23541666666666666\n",
      "Epoch 5, Batch 130, Loss: 1.4034460966403668, Accuracy: 0.23846153846153847\n",
      "Epoch 5, Batch 140, Loss: 1.4001393505505153, Accuracy: 0.24642857142857144\n",
      "Epoch 5, Batch 150, Loss: 1.3997709759076435, Accuracy: 0.25\n",
      "Epoch 5, Batch 160, Loss: 1.4039613351225853, Accuracy: 0.24375\n",
      "Epoch 5, Batch 170, Loss: 1.4055705294889562, Accuracy: 0.24558823529411763\n",
      "Epoch 5, Batch 180, Loss: 1.405178235636817, Accuracy: 0.2465277777777778\n",
      "Epoch 5, Batch 190, Loss: 1.4052854519141347, Accuracy: 0.24539473684210528\n",
      "Epoch 5, Batch 200, Loss: 1.4066413784027099, Accuracy: 0.241875\n",
      "Epoch 5, Batch 210, Loss: 1.4032226778212047, Accuracy: 0.24702380952380953\n",
      "Epoch 5, Batch 220, Loss: 1.4042704809795727, Accuracy: 0.24602272727272728\n",
      "Epoch 5, Batch 230, Loss: 1.4043407865192579, Accuracy: 0.2467391304347826\n",
      "Epoch 5, Batch 240, Loss: 1.4037109792232514, Accuracy: 0.24791666666666667\n",
      "Epoch 5, Batch 250, Loss: 1.4029927082061768, Accuracy: 0.2505\n",
      "Epoch 5, Batch 260, Loss: 1.4025940922590403, Accuracy: 0.25096153846153846\n",
      "Epoch 5, Batch 270, Loss: 1.4024997304987024, Accuracy: 0.25046296296296294\n",
      "Epoch 5, Batch 280, Loss: 1.403966652921268, Accuracy: 0.24732142857142858\n",
      "Epoch 5, Batch 290, Loss: 1.4040279039021195, Accuracy: 0.24698275862068966\n",
      "Epoch 5, Batch 300, Loss: 1.4034847780068715, Accuracy: 0.24916666666666668\n",
      "Epoch 5, Batch 310, Loss: 1.4023839177623871, Accuracy: 0.2524193548387097\n",
      "Epoch 5, Batch 320, Loss: 1.4027035396546126, Accuracy: 0.25234375\n",
      "Epoch 5, Batch 330, Loss: 1.4034001971736099, Accuracy: 0.25227272727272726\n",
      "Epoch 5, Batch 340, Loss: 1.40368632218417, Accuracy: 0.2525735294117647\n",
      "Epoch 5, Batch 350, Loss: 1.403921902860914, Accuracy: 0.2517857142857143\n",
      "Epoch 5, Batch 360, Loss: 1.4034475869602627, Accuracy: 0.25243055555555555\n",
      "Epoch 5, Batch 370, Loss: 1.4041604931290086, Accuracy: 0.25135135135135134\n",
      "Epoch 5, Batch 380, Loss: 1.403491649502202, Accuracy: 0.2523026315789474\n",
      "Epoch 5, Batch 390, Loss: 1.4037690132092207, Accuracy: 0.2512820512820513\n",
      "Epoch 5, Training Loss: 1.403381344940089, Accuracy: 0.25221518987341773\n",
      "Epoch 5, Accuracy: 0.2645569620253165\n",
      "Epoch 6, Batch 10, Loss: 1.422321081161499, Accuracy: 0.2625\n",
      "Epoch 6, Batch 20, Loss: 1.4066263735294342, Accuracy: 0.30625\n",
      "Epoch 6, Batch 30, Loss: 1.4156199137369792, Accuracy: 0.2791666666666667\n",
      "Epoch 6, Batch 40, Loss: 1.410921350121498, Accuracy: 0.275\n",
      "Epoch 6, Batch 50, Loss: 1.40412038564682, Accuracy: 0.2675\n",
      "Epoch 6, Batch 60, Loss: 1.4022187054157258, Accuracy: 0.27291666666666664\n",
      "Epoch 6, Batch 70, Loss: 1.3980627179145813, Accuracy: 0.28035714285714286\n",
      "Epoch 6, Batch 80, Loss: 1.3973007440567016, Accuracy: 0.271875\n",
      "Epoch 6, Batch 90, Loss: 1.4028873801231385, Accuracy: 0.26666666666666666\n",
      "Epoch 6, Batch 100, Loss: 1.4035365772247315, Accuracy: 0.26125\n",
      "Epoch 6, Batch 110, Loss: 1.405321012843739, Accuracy: 0.25795454545454544\n",
      "Epoch 6, Batch 120, Loss: 1.4046226421991983, Accuracy: 0.259375\n",
      "Epoch 6, Batch 130, Loss: 1.401646401331975, Accuracy: 0.2673076923076923\n",
      "Epoch 6, Batch 140, Loss: 1.4066349506378173, Accuracy: 0.2580357142857143\n",
      "Epoch 6, Batch 150, Loss: 1.4058930667241414, Accuracy: 0.2575\n",
      "Epoch 6, Batch 160, Loss: 1.4064164452254773, Accuracy: 0.25625\n",
      "Epoch 6, Batch 170, Loss: 1.4054892876568963, Accuracy: 0.2580882352941177\n",
      "Epoch 6, Batch 180, Loss: 1.4055081943670908, Accuracy: 0.25555555555555554\n",
      "Epoch 6, Batch 190, Loss: 1.4039953909422223, Accuracy: 0.2532894736842105\n",
      "Epoch 6, Batch 200, Loss: 1.4055361115932465, Accuracy: 0.2525\n",
      "Epoch 6, Batch 210, Loss: 1.40494924840473, Accuracy: 0.25595238095238093\n",
      "Epoch 6, Batch 220, Loss: 1.4047465481541372, Accuracy: 0.25795454545454544\n",
      "Epoch 6, Batch 230, Loss: 1.4044616652571638, Accuracy: 0.2597826086956522\n",
      "Epoch 6, Batch 240, Loss: 1.4054453308383623, Accuracy: 0.25885416666666666\n",
      "Epoch 6, Batch 250, Loss: 1.4053646626472474, Accuracy: 0.2565\n",
      "Epoch 6, Batch 260, Loss: 1.4054970984275525, Accuracy: 0.2552884615384615\n",
      "Epoch 6, Batch 270, Loss: 1.4051527027730588, Accuracy: 0.2569444444444444\n",
      "Epoch 6, Batch 280, Loss: 1.4051370923008237, Accuracy: 0.25625\n",
      "Epoch 6, Batch 290, Loss: 1.405460771609997, Accuracy: 0.25474137931034485\n",
      "Epoch 6, Batch 300, Loss: 1.4058279740810393, Accuracy: 0.2520833333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch 310, Loss: 1.406167455257908, Accuracy: 0.25120967741935485\n",
      "Epoch 6, Batch 320, Loss: 1.4051085963845253, Accuracy: 0.253515625\n",
      "Epoch 6, Batch 330, Loss: 1.4052227507938038, Accuracy: 0.25416666666666665\n",
      "Epoch 6, Batch 340, Loss: 1.4056000285288868, Accuracy: 0.2540441176470588\n",
      "Epoch 6, Batch 350, Loss: 1.4046321879114423, Accuracy: 0.2546428571428571\n",
      "Epoch 6, Batch 360, Loss: 1.4034050034152137, Accuracy: 0.25555555555555554\n",
      "Epoch 6, Batch 370, Loss: 1.4034149108706293, Accuracy: 0.25675675675675674\n",
      "Epoch 6, Batch 380, Loss: 1.4034685235274464, Accuracy: 0.25723684210526315\n",
      "Epoch 6, Batch 390, Loss: 1.4031871110965044, Accuracy: 0.25769230769230766\n",
      "Epoch 6, Training Loss: 1.4029029994071285, Accuracy: 0.2572784810126582\n",
      "Epoch 6, Accuracy: 0.2645569620253165\n",
      "Epoch 7, Batch 10, Loss: 1.3482538104057311, Accuracy: 0.35\n",
      "Epoch 7, Batch 20, Loss: 1.367399024963379, Accuracy: 0.33125\n",
      "Epoch 7, Batch 30, Loss: 1.3781915823618571, Accuracy: 0.30416666666666664\n",
      "Epoch 7, Batch 40, Loss: 1.3857383012771607, Accuracy: 0.303125\n",
      "Epoch 7, Batch 50, Loss: 1.393474850654602, Accuracy: 0.285\n",
      "Epoch 7, Batch 60, Loss: 1.3957909882068633, Accuracy: 0.2875\n",
      "Epoch 7, Batch 70, Loss: 1.3952570455414908, Accuracy: 0.2857142857142857\n",
      "Epoch 7, Batch 80, Loss: 1.393207663297653, Accuracy: 0.2859375\n",
      "Epoch 7, Batch 90, Loss: 1.3995535877015857, Accuracy: 0.27361111111111114\n",
      "Epoch 7, Batch 100, Loss: 1.4018630146980287, Accuracy: 0.27\n",
      "Epoch 7, Batch 110, Loss: 1.4018743460828609, Accuracy: 0.26477272727272727\n",
      "Epoch 7, Batch 120, Loss: 1.4026209165652592, Accuracy: 0.259375\n",
      "Epoch 7, Batch 130, Loss: 1.4005426562749423, Accuracy: 0.2701923076923077\n",
      "Epoch 7, Batch 140, Loss: 1.3984873397009714, Accuracy: 0.2767857142857143\n",
      "Epoch 7, Batch 150, Loss: 1.400367735226949, Accuracy: 0.2758333333333333\n",
      "Epoch 7, Batch 160, Loss: 1.4023241996765137, Accuracy: 0.271875\n",
      "Epoch 7, Batch 170, Loss: 1.4029153136646046, Accuracy: 0.2698529411764706\n",
      "Epoch 7, Batch 180, Loss: 1.401072852479087, Accuracy: 0.2708333333333333\n",
      "Epoch 7, Batch 190, Loss: 1.4027522243951496, Accuracy: 0.26907894736842103\n",
      "Epoch 7, Batch 200, Loss: 1.4017381024360658, Accuracy: 0.270625\n",
      "Epoch 7, Batch 210, Loss: 1.4020106173696971, Accuracy: 0.2654761904761905\n",
      "Epoch 7, Batch 220, Loss: 1.4020017017017712, Accuracy: 0.26477272727272727\n",
      "Epoch 7, Batch 230, Loss: 1.4027192680732063, Accuracy: 0.26358695652173914\n",
      "Epoch 7, Batch 240, Loss: 1.401201952000459, Accuracy: 0.26458333333333334\n",
      "Epoch 7, Batch 250, Loss: 1.3992126207351685, Accuracy: 0.269\n",
      "Epoch 7, Batch 260, Loss: 1.3998694878358107, Accuracy: 0.26971153846153845\n",
      "Epoch 7, Batch 270, Loss: 1.401212258250625, Accuracy: 0.2662037037037037\n",
      "Epoch 7, Batch 280, Loss: 1.401154409987586, Accuracy: 0.26607142857142857\n",
      "Epoch 7, Batch 290, Loss: 1.400086914259812, Accuracy: 0.2672413793103448\n",
      "Epoch 7, Batch 300, Loss: 1.399997551838557, Accuracy: 0.26708333333333334\n",
      "Epoch 7, Batch 310, Loss: 1.3998020264410203, Accuracy: 0.2661290322580645\n",
      "Epoch 7, Batch 320, Loss: 1.3989784225821496, Accuracy: 0.265625\n",
      "Epoch 7, Batch 330, Loss: 1.3996863899808942, Accuracy: 0.2636363636363636\n",
      "Epoch 7, Batch 340, Loss: 1.3995744102141436, Accuracy: 0.2665441176470588\n",
      "Epoch 7, Batch 350, Loss: 1.3993190847124373, Accuracy: 0.2671428571428571\n",
      "Epoch 7, Batch 360, Loss: 1.3981197747919294, Accuracy: 0.26805555555555555\n",
      "Epoch 7, Batch 370, Loss: 1.3987762957005887, Accuracy: 0.2668918918918919\n",
      "Epoch 7, Batch 380, Loss: 1.3988085608733327, Accuracy: 0.2654605263157895\n",
      "Epoch 7, Batch 390, Loss: 1.3986261221078726, Accuracy: 0.2644230769230769\n",
      "Epoch 7, Training Loss: 1.3992850807648671, Accuracy: 0.2636075949367089\n",
      "Epoch 7, Accuracy: 0.23417721518987342\n",
      "Epoch 8, Batch 10, Loss: 1.4048982977867126, Accuracy: 0.25\n",
      "Epoch 8, Batch 20, Loss: 1.4122009217739104, Accuracy: 0.2375\n",
      "Epoch 8, Batch 30, Loss: 1.41810751358668, Accuracy: 0.24166666666666667\n",
      "Epoch 8, Batch 40, Loss: 1.408613196015358, Accuracy: 0.275\n",
      "Epoch 8, Batch 50, Loss: 1.4119579958915711, Accuracy: 0.265\n",
      "Epoch 8, Batch 60, Loss: 1.4147821287314097, Accuracy: 0.25416666666666665\n",
      "Epoch 8, Batch 70, Loss: 1.4159262503896441, Accuracy: 0.26071428571428573\n",
      "Epoch 8, Batch 80, Loss: 1.4143686696887017, Accuracy: 0.2609375\n",
      "Epoch 8, Batch 90, Loss: 1.4155489881833394, Accuracy: 0.25972222222222224\n",
      "Epoch 8, Batch 100, Loss: 1.4138754069805146, Accuracy: 0.265\n",
      "Epoch 8, Batch 110, Loss: 1.408667301047932, Accuracy: 0.27045454545454545\n",
      "Epoch 8, Batch 120, Loss: 1.4067353854576747, Accuracy: 0.2708333333333333\n",
      "Epoch 8, Batch 130, Loss: 1.404982744730436, Accuracy: 0.2701923076923077\n",
      "Epoch 8, Batch 140, Loss: 1.4029154854161399, Accuracy: 0.27232142857142855\n",
      "Epoch 8, Batch 150, Loss: 1.4020233368873596, Accuracy: 0.2708333333333333\n",
      "Epoch 8, Batch 160, Loss: 1.4012833498418331, Accuracy: 0.27265625\n",
      "Epoch 8, Batch 170, Loss: 1.4014448053696575, Accuracy: 0.2698529411764706\n",
      "Epoch 8, Batch 180, Loss: 1.3991979976495108, Accuracy: 0.2722222222222222\n",
      "Epoch 8, Batch 190, Loss: 1.4002281772462946, Accuracy: 0.2710526315789474\n",
      "Epoch 8, Batch 200, Loss: 1.400358455181122, Accuracy: 0.27\n",
      "Epoch 8, Batch 210, Loss: 1.4015205127852304, Accuracy: 0.2654761904761905\n",
      "Epoch 8, Batch 220, Loss: 1.3995313368060371, Accuracy: 0.26988636363636365\n",
      "Epoch 8, Batch 230, Loss: 1.3980748539385588, Accuracy: 0.27391304347826084\n",
      "Epoch 8, Batch 240, Loss: 1.3975248366594315, Accuracy: 0.275\n",
      "Epoch 8, Batch 250, Loss: 1.3973003096580505, Accuracy: 0.2735\n",
      "Epoch 8, Batch 260, Loss: 1.3980079151116884, Accuracy: 0.27115384615384613\n",
      "Epoch 8, Batch 270, Loss: 1.3980164695669104, Accuracy: 0.26990740740740743\n",
      "Epoch 8, Batch 280, Loss: 1.398241607631956, Accuracy: 0.27053571428571427\n",
      "Epoch 8, Batch 290, Loss: 1.3979004658501724, Accuracy: 0.2706896551724138\n",
      "Epoch 8, Batch 300, Loss: 1.3975559083620708, Accuracy: 0.26875\n",
      "Epoch 8, Batch 310, Loss: 1.3977743694859166, Accuracy: 0.267741935483871\n",
      "Epoch 8, Batch 320, Loss: 1.3981111727654933, Accuracy: 0.26640625\n",
      "Epoch 8, Batch 330, Loss: 1.3986047329324665, Accuracy: 0.26325757575757575\n",
      "Epoch 8, Batch 340, Loss: 1.3983278972261093, Accuracy: 0.26286764705882354\n",
      "Epoch 8, Batch 350, Loss: 1.3980563851765224, Accuracy: 0.2639285714285714\n",
      "Epoch 8, Batch 360, Loss: 1.3977574424611197, Accuracy: 0.2659722222222222\n",
      "Epoch 8, Batch 370, Loss: 1.3968619527043522, Accuracy: 0.26722972972972975\n",
      "Epoch 8, Batch 380, Loss: 1.396403712347934, Accuracy: 0.26907894736842103\n",
      "Epoch 8, Batch 390, Loss: 1.3960679213205973, Accuracy: 0.2692307692307692\n",
      "Epoch 8, Training Loss: 1.3960838133775735, Accuracy: 0.2680379746835443\n",
      "Epoch 8, Accuracy: 0.23164556962025318\n",
      "Epoch 9, Batch 10, Loss: 1.4408143997192382, Accuracy: 0.2\n",
      "Epoch 9, Batch 20, Loss: 1.4102088689804078, Accuracy: 0.23125\n",
      "Epoch 9, Batch 30, Loss: 1.411474621295929, Accuracy: 0.225\n",
      "Epoch 9, Batch 40, Loss: 1.4086883962154388, Accuracy: 0.228125\n",
      "Epoch 9, Batch 50, Loss: 1.4039690971374512, Accuracy: 0.2375\n",
      "Epoch 9, Batch 60, Loss: 1.3952037950356801, Accuracy: 0.24583333333333332\n",
      "Epoch 9, Batch 70, Loss: 1.3952140927314758, Accuracy: 0.24821428571428572\n",
      "Epoch 9, Batch 80, Loss: 1.3963111907243728, Accuracy: 0.2546875\n",
      "Epoch 9, Batch 90, Loss: 1.3967656837569342, Accuracy: 0.25416666666666665\n",
      "Epoch 9, Batch 100, Loss: 1.396979149580002, Accuracy: 0.25125\n",
      "Epoch 9, Batch 110, Loss: 1.4004691524939104, Accuracy: 0.24772727272727274\n",
      "Epoch 9, Batch 120, Loss: 1.4006281465291976, Accuracy: 0.2520833333333333\n",
      "Epoch 9, Batch 130, Loss: 1.4010668763747582, Accuracy: 0.25384615384615383\n",
      "Epoch 9, Batch 140, Loss: 1.4032697635037559, Accuracy: 0.24910714285714286\n",
      "Epoch 9, Batch 150, Loss: 1.4049522892634074, Accuracy: 0.245\n",
      "Epoch 9, Batch 160, Loss: 1.4056886993348598, Accuracy: 0.24296875\n",
      "Epoch 9, Batch 170, Loss: 1.4074939173810623, Accuracy: 0.24191176470588235\n",
      "Epoch 9, Batch 180, Loss: 1.4093927681446075, Accuracy: 0.24027777777777778\n",
      "Epoch 9, Batch 190, Loss: 1.4094580782087225, Accuracy: 0.2394736842105263\n",
      "Epoch 9, Batch 200, Loss: 1.4090274161100387, Accuracy: 0.23875\n",
      "Epoch 9, Batch 210, Loss: 1.409690204688481, Accuracy: 0.2363095238095238\n",
      "Epoch 9, Batch 220, Loss: 1.4108249068260192, Accuracy: 0.23352272727272727\n",
      "Epoch 9, Batch 230, Loss: 1.4102183093195375, Accuracy: 0.23369565217391305\n",
      "Epoch 9, Batch 240, Loss: 1.4102123672763507, Accuracy: 0.2359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 250, Loss: 1.4078843760490418, Accuracy: 0.2395\n",
      "Epoch 9, Batch 260, Loss: 1.4072089131061847, Accuracy: 0.24134615384615385\n",
      "Epoch 9, Batch 270, Loss: 1.4063642753495111, Accuracy: 0.2412037037037037\n",
      "Epoch 9, Batch 280, Loss: 1.4060553371906281, Accuracy: 0.24285714285714285\n",
      "Epoch 9, Batch 290, Loss: 1.4065282743552636, Accuracy: 0.24267241379310345\n",
      "Epoch 9, Batch 300, Loss: 1.4079483981927237, Accuracy: 0.24041666666666667\n",
      "Epoch 9, Batch 310, Loss: 1.4071797332456035, Accuracy: 0.24233870967741936\n",
      "Epoch 9, Batch 320, Loss: 1.4064304292201997, Accuracy: 0.244921875\n",
      "Epoch 9, Batch 330, Loss: 1.4051096937873147, Accuracy: 0.24772727272727274\n",
      "Epoch 9, Batch 340, Loss: 1.4061615915859447, Accuracy: 0.24889705882352942\n",
      "Epoch 9, Batch 350, Loss: 1.4066531518527439, Accuracy: 0.2475\n",
      "Epoch 9, Batch 360, Loss: 1.4057209644052717, Accuracy: 0.2482638888888889\n",
      "Epoch 9, Batch 370, Loss: 1.40640526655558, Accuracy: 0.24763513513513513\n",
      "Epoch 9, Batch 380, Loss: 1.4056017238842813, Accuracy: 0.2509868421052632\n",
      "Epoch 9, Batch 390, Loss: 1.4055119367746207, Accuracy: 0.2516025641025641\n",
      "Epoch 9, Training Loss: 1.4059630765190607, Accuracy: 0.2509493670886076\n",
      "Epoch 9, Accuracy: 0.2645569620253165\n",
      "Epoch 10, Batch 10, Loss: 1.3817121744155885, Accuracy: 0.325\n",
      "Epoch 10, Batch 20, Loss: 1.3896157026290894, Accuracy: 0.29375\n",
      "Epoch 10, Batch 30, Loss: 1.3941424051920572, Accuracy: 0.2625\n",
      "Epoch 10, Batch 40, Loss: 1.3895012885332108, Accuracy: 0.25625\n",
      "Epoch 10, Batch 50, Loss: 1.3901015949249267, Accuracy: 0.26\n",
      "Epoch 10, Batch 60, Loss: 1.394349640607834, Accuracy: 0.26666666666666666\n",
      "Epoch 10, Batch 70, Loss: 1.395660822732108, Accuracy: 0.26607142857142857\n",
      "Epoch 10, Batch 80, Loss: 1.3984491229057312, Accuracy: 0.2640625\n",
      "Epoch 10, Batch 90, Loss: 1.3997554527388678, Accuracy: 0.25555555555555554\n",
      "Epoch 10, Batch 100, Loss: 1.3986431813240052, Accuracy: 0.255\n",
      "Epoch 10, Batch 110, Loss: 1.3989272919568148, Accuracy: 0.25227272727272726\n",
      "Epoch 10, Batch 120, Loss: 1.3987081031004587, Accuracy: 0.25416666666666665\n",
      "Epoch 10, Batch 130, Loss: 1.398696898497068, Accuracy: 0.2548076923076923\n",
      "Epoch 10, Batch 140, Loss: 1.4011565327644349, Accuracy: 0.25089285714285714\n",
      "Epoch 10, Batch 150, Loss: 1.4004707423845928, Accuracy: 0.245\n",
      "Epoch 10, Batch 160, Loss: 1.3984984055161476, Accuracy: 0.2484375\n",
      "Epoch 10, Batch 170, Loss: 1.3997917652130127, Accuracy: 0.24705882352941178\n",
      "Epoch 10, Batch 180, Loss: 1.4003178093168471, Accuracy: 0.2465277777777778\n",
      "Epoch 10, Batch 190, Loss: 1.402005538187529, Accuracy: 0.24407894736842106\n",
      "Epoch 10, Batch 200, Loss: 1.4020973443984985, Accuracy: 0.245\n",
      "Epoch 10, Batch 210, Loss: 1.4012813443229313, Accuracy: 0.24702380952380953\n",
      "Epoch 10, Batch 220, Loss: 1.4022902006452733, Accuracy: 0.24488636363636362\n",
      "Epoch 10, Batch 230, Loss: 1.4028653782346974, Accuracy: 0.24619565217391304\n",
      "Epoch 10, Batch 240, Loss: 1.404085665444533, Accuracy: 0.24270833333333333\n",
      "Epoch 10, Batch 250, Loss: 1.404063943862915, Accuracy: 0.244\n",
      "Epoch 10, Batch 260, Loss: 1.4038618931403526, Accuracy: 0.24519230769230768\n",
      "Epoch 10, Batch 270, Loss: 1.4034857184798628, Accuracy: 0.24675925925925926\n",
      "Epoch 10, Batch 280, Loss: 1.4026758113077709, Accuracy: 0.24598214285714284\n",
      "Epoch 10, Batch 290, Loss: 1.400952786823799, Accuracy: 0.24913793103448276\n",
      "Epoch 10, Batch 300, Loss: 1.4010743208726246, Accuracy: 0.25\n",
      "Epoch 10, Batch 310, Loss: 1.4016835347298653, Accuracy: 0.2508064516129032\n",
      "Epoch 10, Batch 320, Loss: 1.4023477841168641, Accuracy: 0.2515625\n",
      "Epoch 10, Batch 330, Loss: 1.4017260378057306, Accuracy: 0.2518939393939394\n",
      "Epoch 10, Batch 340, Loss: 1.4006207571310156, Accuracy: 0.25588235294117645\n",
      "Epoch 10, Batch 350, Loss: 1.401392880507878, Accuracy: 0.2542857142857143\n",
      "Epoch 10, Batch 360, Loss: 1.4005412853426404, Accuracy: 0.2565972222222222\n",
      "Epoch 10, Batch 370, Loss: 1.3998603682260256, Accuracy: 0.2570945945945946\n",
      "Epoch 10, Batch 380, Loss: 1.4003713661118558, Accuracy: 0.2565789473684211\n",
      "Epoch 10, Batch 390, Loss: 1.4008700697849958, Accuracy: 0.25416666666666665\n",
      "Epoch 10, Training Loss: 1.4008817452418654, Accuracy: 0.25411392405063293\n",
      "Epoch 10, Accuracy: 0.26962025316455696\n",
      "Epoch 11, Batch 10, Loss: 1.38392174243927, Accuracy: 0.275\n",
      "Epoch 11, Batch 20, Loss: 1.3931817770004273, Accuracy: 0.225\n",
      "Epoch 11, Batch 30, Loss: 1.3993266344070434, Accuracy: 0.22916666666666666\n",
      "Epoch 11, Batch 40, Loss: 1.4054085522890092, Accuracy: 0.215625\n",
      "Epoch 11, Batch 50, Loss: 1.412152042388916, Accuracy: 0.195\n",
      "Epoch 11, Batch 60, Loss: 1.4062601566314696, Accuracy: 0.20208333333333334\n",
      "Epoch 11, Batch 70, Loss: 1.4084189057350158, Accuracy: 0.2017857142857143\n",
      "Epoch 11, Batch 80, Loss: 1.4073835641145707, Accuracy: 0.20625\n",
      "Epoch 11, Batch 90, Loss: 1.4050985667440625, Accuracy: 0.20972222222222223\n",
      "Epoch 11, Batch 100, Loss: 1.4058561873435975, Accuracy: 0.21625\n",
      "Epoch 11, Batch 110, Loss: 1.4112067005851052, Accuracy: 0.21704545454545454\n",
      "Epoch 11, Batch 120, Loss: 1.410016460220019, Accuracy: 0.22395833333333334\n",
      "Epoch 11, Batch 130, Loss: 1.4096766820320716, Accuracy: 0.23173076923076924\n",
      "Epoch 11, Batch 140, Loss: 1.4101204778466905, Accuracy: 0.2357142857142857\n",
      "Epoch 11, Batch 150, Loss: 1.4107966089248658, Accuracy: 0.2375\n",
      "Epoch 11, Batch 160, Loss: 1.4114974699914455, Accuracy: 0.234375\n",
      "Epoch 11, Batch 170, Loss: 1.4103956236558801, Accuracy: 0.23676470588235293\n",
      "Epoch 11, Batch 180, Loss: 1.4111855467160543, Accuracy: 0.23472222222222222\n",
      "Epoch 11, Batch 190, Loss: 1.4083998793049863, Accuracy: 0.2361842105263158\n",
      "Epoch 11, Batch 200, Loss: 1.4069300907850266, Accuracy: 0.240625\n",
      "Epoch 11, Batch 210, Loss: 1.408173181897118, Accuracy: 0.2363095238095238\n",
      "Epoch 11, Batch 220, Loss: 1.4069786369800568, Accuracy: 0.23636363636363636\n",
      "Epoch 11, Batch 230, Loss: 1.4069176943405814, Accuracy: 0.23695652173913043\n",
      "Epoch 11, Batch 240, Loss: 1.4060186182459196, Accuracy: 0.2390625\n",
      "Epoch 11, Batch 250, Loss: 1.404147909641266, Accuracy: 0.2415\n",
      "Epoch 11, Batch 260, Loss: 1.4055363558805907, Accuracy: 0.23846153846153847\n",
      "Epoch 11, Batch 270, Loss: 1.4047196432396218, Accuracy: 0.23981481481481481\n",
      "Epoch 11, Batch 280, Loss: 1.4042799932616097, Accuracy: 0.23973214285714287\n",
      "Epoch 11, Batch 290, Loss: 1.4046575217411437, Accuracy: 0.2418103448275862\n",
      "Epoch 11, Batch 300, Loss: 1.4036497433980306, Accuracy: 0.24333333333333335\n",
      "Epoch 11, Batch 310, Loss: 1.404329627560031, Accuracy: 0.24314516129032257\n",
      "Epoch 11, Batch 320, Loss: 1.4052313707768918, Accuracy: 0.241015625\n",
      "Epoch 11, Batch 330, Loss: 1.4055763815388536, Accuracy: 0.23939393939393938\n",
      "Epoch 11, Batch 340, Loss: 1.405105221271515, Accuracy: 0.23970588235294119\n",
      "Epoch 11, Batch 350, Loss: 1.404898304598672, Accuracy: 0.23892857142857143\n",
      "Epoch 11, Batch 360, Loss: 1.404582988884714, Accuracy: 0.23819444444444443\n",
      "Epoch 11, Batch 370, Loss: 1.4041944867855793, Accuracy: 0.24054054054054055\n",
      "Epoch 11, Batch 380, Loss: 1.4041514462546298, Accuracy: 0.24243421052631578\n",
      "Epoch 11, Batch 390, Loss: 1.4039959751642668, Accuracy: 0.24294871794871795\n",
      "Epoch 11, Training Loss: 1.4036424989941754, Accuracy: 0.24367088607594936\n",
      "Epoch 11, Accuracy: 0.23164556962025318\n",
      "Epoch 12, Batch 10, Loss: 1.4270026206970214, Accuracy: 0.2125\n",
      "Epoch 12, Batch 20, Loss: 1.4231497466564178, Accuracy: 0.23125\n",
      "Epoch 12, Batch 30, Loss: 1.4187723040580749, Accuracy: 0.2375\n",
      "Epoch 12, Batch 40, Loss: 1.4099861532449722, Accuracy: 0.259375\n",
      "Epoch 12, Batch 50, Loss: 1.4098110675811768, Accuracy: 0.255\n",
      "Epoch 12, Batch 60, Loss: 1.4060985505580903, Accuracy: 0.25\n",
      "Epoch 12, Batch 70, Loss: 1.4031004769461495, Accuracy: 0.2517857142857143\n",
      "Epoch 12, Batch 80, Loss: 1.4020001202821732, Accuracy: 0.2546875\n",
      "Epoch 12, Batch 90, Loss: 1.402102706167433, Accuracy: 0.2569444444444444\n",
      "Epoch 12, Batch 100, Loss: 1.4040087974071502, Accuracy: 0.2575\n",
      "Epoch 12, Batch 110, Loss: 1.4030812805349178, Accuracy: 0.2545454545454545\n",
      "Epoch 12, Batch 120, Loss: 1.4027349680662156, Accuracy: 0.25\n",
      "Epoch 12, Batch 130, Loss: 1.4004564046859742, Accuracy: 0.25\n",
      "Epoch 12, Batch 140, Loss: 1.4012306647641317, Accuracy: 0.2517857142857143\n",
      "Epoch 12, Batch 150, Loss: 1.400853338241577, Accuracy: 0.25666666666666665\n",
      "Epoch 12, Batch 160, Loss: 1.4035058423876763, Accuracy: 0.25390625\n",
      "Epoch 12, Batch 170, Loss: 1.4020094906582552, Accuracy: 0.25735294117647056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 180, Loss: 1.401626671685113, Accuracy: 0.25833333333333336\n",
      "Epoch 12, Batch 190, Loss: 1.4011608945695977, Accuracy: 0.2611842105263158\n",
      "Epoch 12, Batch 200, Loss: 1.4000096535682678, Accuracy: 0.263125\n",
      "Epoch 12, Batch 210, Loss: 1.399136063598451, Accuracy: 0.2654761904761905\n",
      "Epoch 12, Batch 220, Loss: 1.3981382749297402, Accuracy: 0.26704545454545453\n",
      "Epoch 12, Batch 230, Loss: 1.3981114200923754, Accuracy: 0.26521739130434785\n",
      "Epoch 12, Batch 240, Loss: 1.3985177829861641, Accuracy: 0.2625\n",
      "Epoch 12, Batch 250, Loss: 1.398637411594391, Accuracy: 0.2595\n",
      "Epoch 12, Batch 260, Loss: 1.3983550933691171, Accuracy: 0.2591346153846154\n",
      "Epoch 12, Batch 270, Loss: 1.3987408227390714, Accuracy: 0.2574074074074074\n",
      "Epoch 12, Batch 280, Loss: 1.3990499049425125, Accuracy: 0.25535714285714284\n",
      "Epoch 12, Batch 290, Loss: 1.397808326112813, Accuracy: 0.25689655172413794\n",
      "Epoch 12, Batch 300, Loss: 1.398215678135554, Accuracy: 0.25583333333333336\n",
      "Epoch 12, Batch 310, Loss: 1.39820826092074, Accuracy: 0.25564516129032255\n",
      "Epoch 12, Batch 320, Loss: 1.3980139713734387, Accuracy: 0.256640625\n",
      "Epoch 12, Batch 330, Loss: 1.3978192289670308, Accuracy: 0.25643939393939397\n",
      "Epoch 12, Batch 340, Loss: 1.3985074926825132, Accuracy: 0.2536764705882353\n",
      "Epoch 12, Batch 350, Loss: 1.3987994381359645, Accuracy: 0.2546428571428571\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/3772714162.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtotal_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 학습 루프\n",
    "for epoch in range(50):  # 예시로 3 에폭만 진행\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[\"input_ids\"],\n",
    "            \"attention_mask\": batch[\"attention_mask\"],\n",
    "            \"labels\": batch[\"label\"],\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += (predictions == batch[\"label\"]).sum().item()\n",
    "        total_samples += len(batch[\"label\"])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 한 batch가 끝날 때마다 출력\n",
    "        if (batch_idx + 1) % 10 == 0:  # 예시로 10 배치마다 출력\n",
    "            average_loss = total_loss / (batch_idx + 1)\n",
    "            accuracy = correct_predictions / total_samples\n",
    "            print(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}, Loss: {average_loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "    # 에폭이 끝날 때마다 출력\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {average_loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "    # 평가 루프\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"],\n",
    "                \"attention_mask\": batch[\"attention_mask\"],\n",
    "                \"labels\": batch[\"label\"],\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            predictions.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "            true_labels.extend(batch[\"label\"].cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Epoch {epoch + 1}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc6a423",
   "metadata": {},
   "source": [
    "---\n",
    "## DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fcde46c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10, Loss: 1.388614284992218, Accuracy: 0.275\n",
      "Epoch 1, Batch 20, Loss: 1.3893455028533936, Accuracy: 0.28125\n",
      "Epoch 1, Batch 30, Loss: 1.3917365789413452, Accuracy: 0.24166666666666667\n",
      "Epoch 1, Batch 40, Loss: 1.3959510445594787, Accuracy: 0.23125\n",
      "Epoch 1, Batch 50, Loss: 1.3981371712684632, Accuracy: 0.225\n",
      "Epoch 1, Batch 60, Loss: 1.397163180510203, Accuracy: 0.23541666666666666\n",
      "Epoch 1, Batch 70, Loss: 1.396534536566053, Accuracy: 0.24285714285714285\n",
      "Epoch 1, Batch 80, Loss: 1.3980304718017578, Accuracy: 0.2359375\n",
      "Epoch 1, Batch 90, Loss: 1.395051650206248, Accuracy: 0.24305555555555555\n",
      "Epoch 1, Batch 100, Loss: 1.3926098370552062, Accuracy: 0.25125\n",
      "Epoch 1, Batch 110, Loss: 1.390545094013214, Accuracy: 0.2534090909090909\n",
      "Epoch 1, Batch 120, Loss: 1.3918190787235896, Accuracy: 0.25\n",
      "Epoch 1, Batch 130, Loss: 1.3917047638159532, Accuracy: 0.25769230769230766\n",
      "Epoch 1, Batch 140, Loss: 1.3910904509680613, Accuracy: 0.2598214285714286\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/2600271625.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mtotal_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 추가\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "# 훈련 데이터셋과 테스트 데이터셋으로 분할\n",
    "train_df, test_df = train_test_split(df_al, test_size=0.2, random_state=42)\n",
    "\n",
    "# DistilBERT 토크나이저 로드\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# 사용자 정의 데이터셋 클래스\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=64):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataframe.iloc[idx]['text']\n",
    "        label = self.dataframe.iloc[idx]['label']\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "train_dataset = CustomDataset(train_df, tokenizer)\n",
    "test_dataset = CustomDataset(test_df, tokenizer)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# DistilBERT 모델 로드\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=df_al['label'].nunique())\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(3):  # 예시로 3 에폭만 진행\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[\"input_ids\"],\n",
    "            \"attention_mask\": batch[\"attention_mask\"],\n",
    "            \"labels\": batch[\"label\"],\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += (predictions == batch[\"label\"]).sum().item()\n",
    "        total_samples += len(batch[\"label\"])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 한 batch가 끝날 때마다 출력\n",
    "        if (batch_idx + 1) % 10 == 0:  # 예시로 10 배치마다 출력\n",
    "            average_loss = total_loss / (batch_idx + 1)\n",
    "            accuracy = correct_predictions / total_samples\n",
    "            print(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}, Loss: {average_loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "    # 에폭이 끝날 때마다 출력\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {average_loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "    # 평가 루프\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"],\n",
    "                \"attention_mask\": batch[\"attention_mask\"],\n",
    "                \"labels\": batch[\"label\"],\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            predictions.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "            true_labels.extend(batch[\"label\"].cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Epoch {epoch + 1}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59197e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb==0.16.0\n",
      "  Downloading wandb-0.16.0-py3-none-any.whl (2.1 MB)\n",
      "     |████████████████████████████████| 2.1 MB 5.3 MB/s            \n",
      "\u001b[?25hCollecting GitPython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
      "     |████████████████████████████████| 196 kB 73.1 MB/s            \n",
      "\u001b[?25hCollecting setproctitle\n",
      "  Downloading setproctitle-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.39.2-py2.py3-none-any.whl (254 kB)\n",
      "     |████████████████████████████████| 254 kB 78.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (6.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (4.0.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (3.19.1)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (5.8.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (8.0.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (1.4.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (59.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (2.26.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb==0.16.0) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "     |████████████████████████████████| 62 kB 2.4 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.16.0) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.16.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.16.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.16.0) (1.26.7)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "     |████████████████████████████████| 143 kB 79.8 MB/s            \n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, urllib3, gitdb, setproctitle, sentry-sdk, GitPython, docker-pycreds, wandb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.7\n",
      "    Uninstalling urllib3-1.26.7:\n",
      "      Successfully uninstalled urllib3-1.26.7\n",
      "Successfully installed GitPython-3.1.41 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.39.2 setproctitle-1.3.3 smmap-5.0.1 urllib3-1.26.18 wandb-0.16.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " !pip install wandb==0.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a927a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d1c34e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx      class                                       conversation\n",
       "0    0      협박 대화  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...\n",
       "1    1      협박 대화  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...\n",
       "2    2  기타 괴롭힘 대화  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...\n",
       "3    3      갈취 대화  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...\n",
       "4    4      갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ..."
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path =\"~/aiffel/dktc/data/train.csv\"\n",
    "df = pd.read_csv(train_data_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "588cb54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = ['협박 대화', '갈취 대화', '직장 내 괴롭힘 대화', '기타 괴롭힘 대화']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(CLASS_NAMES)\n",
    "\n",
    "train_data['class'] = encoder.transform(train_data['class'])\n",
    "\n",
    "corpus = train_data[\"conversation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "83c53335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5527  210 5528 ... 5530   41 5531]\n",
      " [7965 7966    1 ...    1    1  396]\n",
      " [   1   96    1 ...    1  125    1]\n",
      " ...\n",
      " [ 485    3  113 ...   27 2015 5457]\n",
      " [   1    1  152 ...  350    1 3109]\n",
      " [   1    1    1 ...  156  114    1]] <keras_preprocessing.text.Tokenizer object at 0x7f49301a7b50>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=10000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=20)  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e88cc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : 내가\n",
      "3 : 너\n",
      "4 : 좀\n",
      "5 : 다\n",
      "6 : 왜\n",
      "7 : 나\n",
      "8 : 진짜\n",
      "9 : 야\n",
      "10 : 지금\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dbb28d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tensor[:3500]\n",
    "y_train = train_data['class'][:3500]\n",
    "X_val = tensor[3500:3850]\n",
    "y_val = train_data['class'][3500:3850]\n",
    "X_test = tensor[3850:]\n",
    "y_test = train_data['class'][3850:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "304eba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "edd498ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>3945</td>\n",
       "      <td>1</td>\n",
       "      <td>준하야 넌 대가리가 왜이렇게 크냐?\\n내 머리가 뭐.\\n밥먹으면 대가리만 크냐 너는...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>3946</td>\n",
       "      <td>0</td>\n",
       "      <td>내가 지금 너 아들 김길준 데리고 있어. 살리고 싶으면 계좌에 1억만 보내\\n예.?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>3947</td>\n",
       "      <td>2</td>\n",
       "      <td>나는 씨 같은 사람 보면 참 신기하더라. 어떻게 저렇게 살지.\\n왜 그래. 들리겠어...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>3948</td>\n",
       "      <td>0</td>\n",
       "      <td>누구맘대로 여기서 장사하래?\\n이게 무슨일입니까?\\n남의 구역에서 장사하려면 자릿세...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3949</th>\n",
       "      <td>3949</td>\n",
       "      <td>2</td>\n",
       "      <td>희정씨\\n네?\\n주말에 시간이 넘쳐나나봐\\n갑자기 왜그러세요?\\n손이 빤짝빤짝 네일...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3950 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx  class                                       conversation\n",
       "0        0      3  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...\n",
       "1        1      3  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...\n",
       "2        2      1  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...\n",
       "3        3      0  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...\n",
       "4        4      0  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...\n",
       "...    ...    ...                                                ...\n",
       "3945  3945      1  준하야 넌 대가리가 왜이렇게 크냐?\\n내 머리가 뭐.\\n밥먹으면 대가리만 크냐 너는...\n",
       "3946  3946      0  내가 지금 너 아들 김길준 데리고 있어. 살리고 싶으면 계좌에 1억만 보내\\n예.?...\n",
       "3947  3947      2  나는 씨 같은 사람 보면 참 신기하더라. 어떻게 저렇게 살지.\\n왜 그래. 들리겠어...\n",
       "3948  3948      0  누구맘대로 여기서 장사하래?\\n이게 무슨일입니까?\\n남의 구역에서 장사하려면 자릿세...\n",
       "3949  3949      2  희정씨\\n네?\\n주말에 시간이 넘쳐나나봐\\n갑자기 왜그러세요?\\n손이 빤짝빤짝 네일...\n",
       "\n",
       "[3950 rows x 3 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = train_data\n",
    "df.loc[ df['class']=='협박 대화', 'idx'] = 0\n",
    "df.loc[ df['class']=='기타 괴롭힘 대화', 'idx'] = 3\n",
    "df.loc[ df['class']=='갈취 대화', 'idx'] = 1\n",
    "df.loc[ df['class']=='직장 내 괴롭힘 대화', 'idx'] = 2\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0db53da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...      3\n",
       "1  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...      3\n",
       "2  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...      1\n",
       "3  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...      0\n",
       "4  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...      0"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['conversation','class']].rename(columns = {'class':'label','conversation':'text'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6e4379e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "28a331ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_projector', 'activation_13', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "config = transformers.DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
    "dbert_tf = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "87ae724a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data:  (3160, 225)\n",
      "Shape of test data:  (790, 225)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_projector', 'activation_13', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "X= df['text']\n",
    "y = df['label']\n",
    "# DistilBertTokenizer로 모든 문장 토큰화\n",
    "X_tf = [tokenizer(text, padding='max_length', max_length = 225, truncation=True)['input_ids'] for text in X]\n",
    "X_tf = np.array(X_tf, dtype='int32')\n",
    "# Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tf, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print('Shape of training data: ',X_train.shape)\n",
    "print('Shape of test data: ',X_test.shape)\n",
    "# Get BERT layer\n",
    "import transformers\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "config = transformers.DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
    "dbert_tf = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "410a0816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /aiffel/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key = '0d59d7a17cbfbc47091fd5af0f49a1908e832c2e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f5d26aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"name\": \"sweep_test_nlp\",\n",
    "    \"metric\": {\"name\": \"val_loss\", \"goal\": \"minimize\"},\n",
    "    \"method\": \"random\",\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\" : {\n",
    "            \"min\" : 0.001,\n",
    "            \"max\" : 0.1\n",
    "            },\n",
    "        \"epoch\" : {\n",
    "            \"distribution\" : \"int_uniform\",\n",
    "            \"min\" : 5,\n",
    "            \"max\" : 10\n",
    "            }\n",
    "                    \n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "2eebe62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "from tensorflow.keras import models, layers, metrics\n",
    "\n",
    "def train():\n",
    "    default_config = {\n",
    "    \"vocab\": 10000,\n",
    "    \"units_1\": 64,\n",
    "    \"units_2\": 32,\n",
    "    \"units_3\" : 32,\n",
    "    \"class_num\": 4,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"sparse_categorical_crossentropy\",\n",
    "    \"metrics\": [\"accuracy\"],\n",
    "    \"epoch\": 10,\n",
    "    \"batch_size\": 16,\n",
    "    \"dropout\": 0.5\n",
    "    }\n",
    "\n",
    "    wandb.init(config = default_config)\n",
    "    config = wandb.config\n",
    "\n",
    "    # Model\n",
    "    # Get BERT layer\n",
    "    # 입력 정의\n",
    "      # Model\n",
    "    #input_ids_in = layers.Input(shape=(max_length,), dtype='int32')\n",
    "    #attention_mask_in = layers.Input(shape=(max_length,), dtype='int32')\n",
    "\n",
    "    input_ids_in = keras.layers.Input(shape=(225,), name='input_token', dtype='int32')\n",
    "\n",
    "    x = dbert_tf(input_ids=input_ids_in)[0][:,0,:]\n",
    "    x = keras.layers.Dropout(config['dropout'], name='dropout')(x)\n",
    "    x = keras.layers.Dense(config['units_1'], activation='relu')(x)   \n",
    "    #x = keras.layers.LSTM(units=config['units_2'])(x)\n",
    "    x = keras.layers.Dense(config['class_num'], activation='softmax', name='classification')(x)\n",
    "\n",
    "    model = models.Model(inputs=input_ids_in, outputs = x, name='ClassificationModelTF')\n",
    "\n",
    "\n",
    "    # Concatenate DistilBERT output and LSTM output\n",
    "    #combined_model = layers.concatenate([distilbert_output, lstm_model.output])\n",
    "    #output_layer = layers.Dense(config['class_num'], activation='softmax')(combined_model)\n",
    "\n",
    "    # Final Model\n",
    "    #model = keras.Model(inputs=[input_ids_in, attention_mask_in], outputs=output_layer)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss=config['loss'], metrics=config['metrics'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        epochs=config['epoch'],\n",
    "        batch_size=config['batch_size'],\n",
    "        validation_split=0.2,\n",
    "        callbacks=[WandbCallback()]\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "    # Log test metrics to WandB\n",
    "    wandb.log({\"Test Accuracy Rate\": round(test_accuracy * 100, 2),\n",
    "               \"Test Error Rate\": round((1 - test_accuracy) * 100, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4929e3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: o4ls6xsh\n",
      "Sweep URL: https://wandb.ai/yunjiyeong0106/aiffel/sweeps/o4ls6xsh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w4fg8wp7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.05146482669207599\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/aiffel/aiffel/Aiffel_online_Quest/DL/wandb/run-20240110_152714-w4fg8wp7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yunjiyeong0106/aiffel/runs/w4fg8wp7' target=\"_blank\">astral-sweep-1</a></strong> to <a href='https://wandb.ai/yunjiyeong0106/aiffel' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/yunjiyeong0106/aiffel/sweeps/o4ls6xsh' target=\"_blank\">https://wandb.ai/yunjiyeong0106/aiffel/sweeps/o4ls6xsh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yunjiyeong0106/aiffel' target=\"_blank\">https://wandb.ai/yunjiyeong0106/aiffel</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/yunjiyeong0106/aiffel/sweeps/o4ls6xsh' target=\"_blank\">https://wandb.ai/yunjiyeong0106/aiffel/sweeps/o4ls6xsh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yunjiyeong0106/aiffel/runs/w4fg8wp7' target=\"_blank\">https://wandb.ai/yunjiyeong0106/aiffel/runs/w4fg8wp7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  6/158 [>.............................] - ETA: 19s - loss: 11.7039 - accuracy: 0.2604WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_train_batch_end` time: 0.1296s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_train_batch_end` time: 0.1296s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 31s 176ms/step - loss: 1.8082 - accuracy: 0.2532 - val_loss: 1.3927 - val_accuracy: 0.2563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model in the h5py format. The model will be saved as as an W&B Artifact in the 'tf' format.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49302418b0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49302418b0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a006916d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a006916d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f495865ad00>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f495865ad00>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a00280cd0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a00280cd0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a0feded90>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a0feded90>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49447d4fd0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49447d4fd0>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, transformer_layer_call_and_return_conditional_losses, transformer_layer_call_fn, add_8_layer_call_and_return_conditional_losses while saving (showing 5 of 415). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /aiffel/aiffel/Aiffel_online_Quest/DL/wandb/run-20240110_152714-w4fg8wp7/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /aiffel/aiffel/Aiffel_online_Quest/DL/wandb/run-20240110_152714-w4fg8wp7/files/model-best/assets\n",
      "/opt/conda/lib/python3.9/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/aiffel/aiffel/Aiffel_online_Quest/DL/wandb/run-20240110_152714-w4fg8wp7/files/model-best)... Done. 3.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "158/158 [==============================] - 26s 164ms/step - loss: 1.3855 - accuracy: 0.2745 - val_loss: 1.3893 - val_accuracy: 0.2563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model in the h5py format. The model will be saved as as an W&B Artifact in the 'tf' format.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49302418b0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49302418b0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a006916d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a006916d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f495865ad00>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f495865ad00>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a00280cd0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a00280cd0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a0feded90>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a0feded90>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49447d4fd0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49447d4fd0>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, transformer_layer_call_and_return_conditional_losses, transformer_layer_call_fn, add_8_layer_call_and_return_conditional_losses while saving (showing 5 of 415). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /aiffel/aiffel/Aiffel_online_Quest/DL/wandb/run-20240110_152714-w4fg8wp7/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /aiffel/aiffel/Aiffel_online_Quest/DL/wandb/run-20240110_152714-w4fg8wp7/files/model-best/assets\n",
      "/opt/conda/lib/python3.9/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/aiffel/aiffel/Aiffel_online_Quest/DL/wandb/run-20240110_152714-w4fg8wp7/files/model-best)... Done. 4.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "158/158 [==============================] - 26s 166ms/step - loss: 1.3865 - accuracy: 0.2816 - val_loss: 1.3969 - val_accuracy: 0.2563\n",
      "Epoch 4/10\n",
      "158/158 [==============================] - 26s 165ms/step - loss: 1.3875 - accuracy: 0.2718 - val_loss: 1.3895 - val_accuracy: 0.2706\n",
      "Epoch 5/10\n",
      "158/158 [==============================] - 26s 162ms/step - loss: 1.3878 - accuracy: 0.2627 - val_loss: 1.4001 - val_accuracy: 0.2563\n",
      "Epoch 6/10\n",
      "158/158 [==============================] - 26s 164ms/step - loss: 1.3879 - accuracy: 0.2722 - val_loss: 1.3970 - val_accuracy: 0.2563\n",
      "Epoch 7/10\n",
      "158/158 [==============================] - 26s 164ms/step - loss: 1.3872 - accuracy: 0.2674 - val_loss: 1.4006 - val_accuracy: 0.2563\n",
      "Epoch 8/10\n",
      "158/158 [==============================] - 26s 163ms/step - loss: 1.3890 - accuracy: 0.2745 - val_loss: 1.4042 - val_accuracy: 0.2563\n",
      "Epoch 9/10\n",
      "158/158 [==============================] - 26s 163ms/step - loss: 1.3902 - accuracy: 0.2611 - val_loss: 1.3870 - val_accuracy: 0.2706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model in the h5py format. The model will be saved as as an W&B Artifact in the 'tf' format.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49302418b0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49302418b0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a006916d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a006916d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f495865ad00>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f495865ad00>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a00280cd0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a00280cd0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a0feded90>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a0feded90>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49447d4fd0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49447d4fd0>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, transformer_layer_call_and_return_conditional_losses, transformer_layer_call_fn, add_8_layer_call_and_return_conditional_losses while saving (showing 5 of 415). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /aiffel/aiffel/Aiffel_online_Quest/DL/wandb/run-20240110_152714-w4fg8wp7/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /aiffel/aiffel/Aiffel_online_Quest/DL/wandb/run-20240110_152714-w4fg8wp7/files/model-best/assets\n",
      "/opt/conda/lib/python3.9/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/aiffel/aiffel/Aiffel_online_Quest/DL/wandb/run-20240110_152714-w4fg8wp7/files/model-best)... Done. 4.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "158/158 [==============================] - 26s 165ms/step - loss: 1.3895 - accuracy: 0.2666 - val_loss: 1.3869 - val_accuracy: 0.2563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model in the h5py format. The model will be saved as as an W&B Artifact in the 'tf' format.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49302418b0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49302418b0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a006916d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a006916d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f495865ad00>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f495865ad00>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a00280cd0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a00280cd0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a0feded90>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a0feded90>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49447d4fd0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49447d4fd0>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, transformer_layer_call_and_return_conditional_losses, transformer_layer_call_fn, add_8_layer_call_and_return_conditional_losses while saving (showing 5 of 415). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /aiffel/aiffel/Aiffel_online_Quest/DL/wandb/run-20240110_152714-w4fg8wp7/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /aiffel/aiffel/Aiffel_online_Quest/DL/wandb/run-20240110_152714-w4fg8wp7/files/model-best/assets\n",
      "/opt/conda/lib/python3.9/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/aiffel/aiffel/Aiffel_online_Quest/DL/wandb/run-20240110_152714-w4fg8wp7/files/model-best)... Done. 4.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 6s - loss: 1.3857 - accuracy: 0.2772\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1044.463 MB of 1044.463 MB uploaded (0.215 MB deduped)\\r'), FloatProgress(value=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy Rate</td><td>▁</td></tr><tr><td>Test Error Rate</td><td>▁</td></tr><tr><td>accuracy</td><td>▁▆█▆▃▆▅▆▃▄</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁█▁▁▁▁█▁</td></tr><tr><td>val_loss</td><td>▃▂▅▂▆▅▇█▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy Rate</td><td>27.72</td></tr><tr><td>Test Error Rate</td><td>72.28</td></tr><tr><td>accuracy</td><td>0.26661</td></tr><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>1.38685</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>1.38946</td></tr><tr><td>val_accuracy</td><td>0.25633</td></tr><tr><td>val_loss</td><td>1.38685</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">astral-sweep-1</strong> at: <a href='https://wandb.ai/yunjiyeong0106/aiffel/runs/w4fg8wp7' target=\"_blank\">https://wandb.ai/yunjiyeong0106/aiffel/runs/w4fg8wp7</a><br/>Synced 5 W&B file(s), 1 media file(s), 16 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240110_152714-w4fg8wp7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0xjus0z7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.053508129688293445\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/aiffel/aiffel/Aiffel_online_Quest/DL/wandb/run-20240110_153339-0xjus0z7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yunjiyeong0106/aiffel/runs/0xjus0z7' target=\"_blank\">major-sweep-2</a></strong> to <a href='https://wandb.ai/yunjiyeong0106/aiffel' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/yunjiyeong0106/aiffel/sweeps/o4ls6xsh' target=\"_blank\">https://wandb.ai/yunjiyeong0106/aiffel/sweeps/o4ls6xsh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yunjiyeong0106/aiffel' target=\"_blank\">https://wandb.ai/yunjiyeong0106/aiffel</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/yunjiyeong0106/aiffel/sweeps/o4ls6xsh' target=\"_blank\">https://wandb.ai/yunjiyeong0106/aiffel/sweeps/o4ls6xsh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yunjiyeong0106/aiffel/runs/0xjus0z7' target=\"_blank\">https://wandb.ai/yunjiyeong0106/aiffel/runs/0xjus0z7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "  6/158 [>.............................] - ETA: 20s - loss: 5.9267 - accuracy: 0.3229WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_train_batch_end` time: 0.1332s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_train_batch_end` time: 0.1332s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 31s 176ms/step - loss: 1.5919 - accuracy: 0.2702 - val_loss: 1.3896 - val_accuracy: 0.2563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model in the h5py format. The model will be saved as as an W&B Artifact in the 'tf' format.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49302418b0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49302418b0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a006916d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a006916d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f495865ad00>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f495865ad00>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a00280cd0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a00280cd0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a0feded90>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f4a0feded90>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49447d4fd0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7f49447d4fd0>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, transformer_layer_call_and_return_conditional_losses, transformer_layer_call_fn, add_8_layer_call_and_return_conditional_losses while saving (showing 5 of 415). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /aiffel/aiffel/Aiffel_online_Quest/DL/wandb/run-20240110_153339-0xjus0z7/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /aiffel/aiffel/Aiffel_online_Quest/DL/wandb/run-20240110_153339-0xjus0z7/files/model-best/assets\n",
      "/opt/conda/lib/python3.9/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/aiffel/aiffel/Aiffel_online_Quest/DL/wandb/run-20240110_153339-0xjus0z7/files/model-best)... Done. 3.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/6\n",
      "158/158 [==============================] - 26s 163ms/step - loss: 1.3883 - accuracy: 0.2599 - val_loss: 1.3925 - val_accuracy: 0.2389\n",
      "Epoch 3/6\n",
      "158/158 [==============================] - 26s 167ms/step - loss: 1.3896 - accuracy: 0.2627 - val_loss: 1.3955 - val_accuracy: 0.2563\n",
      "Epoch 4/6\n",
      "158/158 [==============================] - 26s 162ms/step - loss: 1.3895 - accuracy: 0.2611 - val_loss: 1.4005 - val_accuracy: 0.2563\n",
      "Epoch 5/6\n",
      "158/158 [==============================] - 26s 163ms/step - loss: 1.3865 - accuracy: 0.2686 - val_loss: 1.3973 - val_accuracy: 0.2389\n",
      "Epoch 6/6\n",
      "158/158 [==============================] - 26s 165ms/step - loss: 1.3868 - accuracy: 0.2714 - val_loss: 1.3912 - val_accuracy: 0.2389\n",
      "25/25 - 6s - loss: 1.3901 - accuracy: 0.2481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "# entity와 project에 본인의 아이디와 프로젝트명을 입력하세요\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config,\n",
    "                       entity = 'yunjiyeong0106',\n",
    "                       project = 'aiffel')\n",
    "\n",
    "# run the sweep\n",
    "wandb.agent(sweep_id,\n",
    "            function=train,\n",
    "            count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160df133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
