{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48980050",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb==0.16.0 in /opt/conda/lib/python3.9/site-packages (0.16.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (3.19.1)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (1.3.3)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (6.0)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (3.1.41)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (0.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (2.26.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (8.0.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (59.4.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (4.0.1)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (1.39.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (5.8.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.9/site-packages (from wandb==0.16.0) (1.4.4)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb==0.16.0) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.16.0) (4.0.11)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.16.0) (1.26.18)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.16.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.16.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.16.0) (2.0.8)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.16.0) (5.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb==0.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f751ebbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx      class                                       conversation\n",
       "0    0      협박 대화  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...\n",
       "1    1      협박 대화  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...\n",
       "2    2  기타 괴롭힘 대화  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...\n",
       "3    3      갈취 대화  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...\n",
       "4    4      갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "train_data_path = os.getenv('HOME')+\"/aiffel/dktc/data/train.csv\"\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f432ee9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>3</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>준하야 넌 대가리가 왜이렇게 크냐?\\n내 머리가 뭐.\\n밥먹으면 대가리만 크냐 너는...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>1</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>내가 지금 너 아들 김길준 데리고 있어. 살리고 싶으면 계좌에 1억만 보내\\n예.?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>나는 씨 같은 사람 보면 참 신기하더라. 어떻게 저렇게 살지.\\n왜 그래. 들리겠어...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>1</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>누구맘대로 여기서 장사하래?\\n이게 무슨일입니까?\\n남의 구역에서 장사하려면 자릿세...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3949</th>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>희정씨\\n네?\\n주말에 시간이 넘쳐나나봐\\n갑자기 왜그러세요?\\n손이 빤짝빤짝 네일...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3950 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      idx        class                                       conversation\n",
       "0       0        협박 대화  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...\n",
       "1       0        협박 대화  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...\n",
       "2       3    기타 괴롭힘 대화  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...\n",
       "3       1        갈취 대화  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...\n",
       "4       1        갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...\n",
       "...   ...          ...                                                ...\n",
       "3945    3    기타 괴롭힘 대화  준하야 넌 대가리가 왜이렇게 크냐?\\n내 머리가 뭐.\\n밥먹으면 대가리만 크냐 너는...\n",
       "3946    1        갈취 대화  내가 지금 너 아들 김길준 데리고 있어. 살리고 싶으면 계좌에 1억만 보내\\n예.?...\n",
       "3947    2  직장 내 괴롭힘 대화  나는 씨 같은 사람 보면 참 신기하더라. 어떻게 저렇게 살지.\\n왜 그래. 들리겠어...\n",
       "3948    1        갈취 대화  누구맘대로 여기서 장사하래?\\n이게 무슨일입니까?\\n남의 구역에서 장사하려면 자릿세...\n",
       "3949    2  직장 내 괴롭힘 대화  희정씨\\n네?\\n주말에 시간이 넘쳐나나봐\\n갑자기 왜그러세요?\\n손이 빤짝빤짝 네일...\n",
       "\n",
       "[3950 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_al = train_data\n",
    "df_al.loc[ df_al['class']=='협박 대화', 'idx'] = 0\n",
    "df_al.loc[ df_al['class']=='기타 괴롭힘 대화', 'idx'] = 3\n",
    "df_al.loc[ df_al['class']=='갈취 대화', 'idx'] = 1\n",
    "df_al.loc[ df_al['class']=='직장 내 괴롭힘 대화', 'idx'] = 2\n",
    "\n",
    "df_al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fbdd928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx                                       conversation\n",
       "0    0  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_basic = df_al[['idx','conversation']]\n",
    "df_basic.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12bdd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sen in cleaned_corpus:\n",
    "    length = len(sen)\n",
    "    if min_len > length: min_len = length\n",
    "    if max_len < length: max_len = length\n",
    "    sum_len += length\n",
    "\n",
    "print(\"문장의 최단 길이:\", min_len)\n",
    "print(\"문장의 최장 길이:\", max_len)\n",
    "print(\"문장의 평균 길이:\", sum_len // len(cleaned_corpus))\n",
    "\n",
    "sentence_length = np.zeros((max_len), dtype=int)\n",
    "\n",
    "for sen in cleaned_corpus:   # 중복이 제거된 코퍼스 기준\n",
    "    sentence_length[len(sen)-1] += 1\n",
    "\n",
    "plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbc9efa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 위치 바꾸기\n",
    "def shuffle_tokens(sentence):\n",
    "    shuffled_sentence = sentence.copy()\n",
    "    random.shuffle(shuffled_sentence)\n",
    "    return shuffled_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fea94a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence, is_english=True):\n",
    "    # 한국어 불용어 list\n",
    "    stopwords_kr = ['의', '가','로', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한','가량',  '간', '갖은', '개국', '개년', '개소', '개월', '걔', '거', ]\n",
    "    #Lucene = '거기, 건, 것, 겨를, 격, 겸, 고, 군, 군데, 권, 그, 그거, 그것, 그곳, 그까짓, 그네,  그놈, 그대, 그래, 그래도,그서, 그러나, 그러니, 그러니까, 그러다가, 그러면, 그러면서, 그러므로, 그러자, 그런, 그런데, 그럼, 그렇지만, 그루, 그리고, 그리하여, 그분, 그이, 그쪽, 근, 근데, 글쎄, 글쎄요, 기,김, 나름, 나위, 남짓,  냥, 너, 너희, 네, 네놈, 녀석, 년대, 년도, 놈, 누구, 니,다른, 다만, 단, 달 대, 대로, 더,구나, 더욱이, 데, 도, 동, 되, 두, 두세, 두어,둥, 듯, 듯이, 등, 등등, 등지, 따라서, 따름, 따위, 딴, 때문, 또, 또는, 또한, 리, 마당, 마련,마리, 만, 만큼, 말, 맨, 몇, 몇몇, 모, 모금, 모든, 무렵, 무슨, 뭐, 뭣, 미터,및, 바, 박, 발, 발짝, 번, 법, 별, 본, 부, 분, 뻔, 뿐, 살,서너, 석, 설,섬, 세, 세기, 셈, 쇤네, 수, 순, 스무, 승, 시, 시간, 식, 씨, 아, 아냐, 아니,아무개, 아무런, 아아, 아이, 아이고, 아이구, 야, 약, 양, 얘, 어,  어머, 에이,엔, 여기, 여느, 여보, 여보세요, 여지, 역시, 예, 옛, 오, 오랜, 오히려, 온, 온갖,올, 왜냐하면, 왠, 외, 요, 우리, 원, 월, 웬, 위, 음, 응, 이, 이거, 이것, 이곳, 이놈, 이래,이런, 이런저런, 이른바, 이리하여, 이쪽, 일, 일대, 임마, 자, 자기, 자네, 장, 저, 저것, 저기,저놈, 저런, 저쪽, 저편, 저희, 적, 전, 점, 제, 조, 주, 주년, 주일, 줄, 중, 즈음, 즉, 지, 지경, 짝, 쪽, 쯤, 차, 참, 채, 척, 첫, 체, 초, 총, 측, 치, 타, 터, 턱, 톤,통, 투, 판, 퍼센트, 편, 평, 푼, 하기야, 하긴, 하물며, 하지만, 한, 한두, 한편, 허허, 헌, 현,호, 회, 흥'.split(', ')\n",
    "    #stopwords_kr += Lucene\n",
    "\n",
    "\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    sentence = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]+\", \"\", sentence)\n",
    "    sentence = mecab.morphs(sentence)\n",
    "    sentence = [word for word in sentence if word not in stopwords_kr]\n",
    "    sentence = shuffle_tokens(sentence)\n",
    "    sentence = ' '.join(sentence)\n",
    "\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c494619b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인가 죄송 에게 을래 겠 네 너 죽여 고 죽여 것 지 싶 가족 모조리 습니다 하 달 버려야 겠 너 버리 군 못 까지 을 해 습니다 너 버릴 선택 권 죽 을 잘못 없 해 죽여 어 정말 애원 못 선택 했 만 합니다 너 진짜 아닙니다 네 세요 정말 다 야 지금 제발 죽여 그냥 도와 주 번 너 한다면 휘말리 세요 주 없 가족 도와 죽여 지 죽 죄송 까지 했 혼자 죽 사건 게 합니다 하 면 줄까 습니다 게 라고 선택 선택 스스로 우리 잘못', '터지 분 다 경고 더 세요 얘기 시 장난 장난 네 지 있 재미있 걸 마트 말 마트 해당 번 이야기 방해죄 시 폭발물 했 야 경찰서 전화 야 난 지 다는 선생 꽤 달려 폭발물 길동 한다 길동 터지 판단 전화 을 면 됩니다 할 겠 다 똑바로 입니다 마시 만 분 죽 어 폭발물 어 업무 에게 하 나 설치 죠 님 면 경찰서 했 너 진정 다', '있 나 놀리 봐 지영 키 지 네 어 어 고 안 좋 어 그거 그 해 이상 대잖아 되게 겠 괴롭히 봤 장가가 니 던데 을 그만 작 지 귀여운 키 아 안 크 데 줬 아 어찌 나람 너 살 니 안 첨 주 작 저 네 남자 라 제발 그럼 돼 고 보다 너 재 있 군대 니 알 너 너 나 난쟁이 해 엄마 그만 하 키 준 더 마 희망 라고 냐 해 지 다 가져 하 보태 디 재미없 수술']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "import random\n",
    "\n",
    "mecab = Mecab()\n",
    "corpus = df_al[['conversation']].values\n",
    "copus_aug = [preprocessing(c[0], is_english=True) for c in corpus]\n",
    "print(copus_aug[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31dc7ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>인가 죄송 에게 을래 겠 네 너 죽여 고 죽여 것 지 싶 가족 모조리 습니다 하 달...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>터지 분 다 경고 더 세요 얘기 시 장난 장난 네 지 있 재미있 걸 마트 말 마트 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>있 나 놀리 봐 지영 키 지 네 어 어 고 안 좋 어 그거 그 해 이상 대잖아 되게...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>이리 죽 면 거기 에요 뒤져서 돈 예 기회 오늘 피시방 라고 무슨 나오 있 너 봐 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>날 신가 지 누구 잖아요 된다고 아 하 예민 아니 요 누구 실래요 마세요 저기요 다...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx                                       conversation\n",
       "0    0  인가 죄송 에게 을래 겠 네 너 죽여 고 죽여 것 지 싶 가족 모조리 습니다 하 달...\n",
       "1    0  터지 분 다 경고 더 세요 얘기 시 장난 장난 네 지 있 재미있 걸 마트 말 마트 ...\n",
       "2    3  있 나 놀리 봐 지영 키 지 네 어 어 고 안 좋 어 그거 그 해 이상 대잖아 되게...\n",
       "3    1  이리 죽 면 거기 에요 뒤져서 돈 예 기회 오늘 피시방 라고 무슨 나오 있 너 봐 ...\n",
       "4    1  날 신가 지 누구 잖아요 된다고 아 하 예민 아니 요 누구 실래요 마세요 저기요 다..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame(df_al['idx'].values, columns = ['idx'])\n",
    "new_df['conversation'] = copus_aug\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c69ebc93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>인가 죄송 에게 을래 겠 네 너 죽여 고 죽여 것 지 싶 가족 모조리 습니다 하 달...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>터지 분 다 경고 더 세요 얘기 시 장난 장난 네 지 있 재미있 걸 마트 말 마트 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>있 나 놀리 봐 지영 키 지 네 어 어 고 안 좋 어 그거 그 해 이상 대잖아 되게...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>이리 죽 면 거기 에요 뒤져서 돈 예 기회 오늘 피시방 라고 무슨 나오 있 너 봐 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>날 신가 지 누구 잖아요 된다고 아 하 예민 아니 요 누구 실래요 마세요 저기요 다...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx                                       conversation\n",
       "0    0  인가 죄송 에게 을래 겠 네 너 죽여 고 죽여 것 지 싶 가족 모조리 습니다 하 달...\n",
       "1    0  터지 분 다 경고 더 세요 얘기 시 장난 장난 네 지 있 재미있 걸 마트 말 마트 ...\n",
       "2    3  있 나 놀리 봐 지영 키 지 네 어 어 고 안 좋 어 그거 그 해 이상 대잖아 되게...\n",
       "3    1  이리 죽 면 거기 에요 뒤져서 돈 예 기회 오늘 피시방 라고 무슨 나오 있 너 봐 ...\n",
       "4    1  날 신가 지 누구 잖아요 된다고 아 하 예민 아니 요 누구 실래요 마세요 저기요 다..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_al_new = pd.concat([new_df, df_basic], ignore_index=True)\n",
    "df_al_new=df_al_new.drop_duplicates()\n",
    "df_al_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b82c2d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_al=df_al.rename(columns={'idx':'label','conversation':'text'}).drop('class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "335dee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_al=df_al_new.rename(columns={'idx':'label','conversation':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac3aeca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at kykim/bert-kor-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 추가\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "# 훈련 데이터셋과 테스트 데이터셋으로 분할\n",
    "train_df, test_df = train_test_split(df_al, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"kykim/bert-kor-base\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"kykim/bert-kor-base\", num_labels=len(df_al['label'].unique()))\n",
    "\n",
    "# 사용자 정의 데이터셋 클래스\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataframe.iloc[idx]['text']\n",
    "        label = self.dataframe.iloc[idx]['label']\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "train_dataset = CustomDataset(train_df, tokenizer)\n",
    "test_dataset = CustomDataset(test_df, tokenizer)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# WandB sweep 설정\n",
    "sweep_config = {\n",
    "    \"name\": \"sweep_test_5\",\n",
    "    \"metric\": {\"name\": \"val_loss\", \"goal\": \"minimize\"},\n",
    "    \"method\": \"random\",\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"min\": 0.0001,\n",
    "            \"max\": 0.005\n",
    "        },\n",
    "        \"epoch\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"min\": 3,\n",
    "            \"max\": 5\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fba8f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myunjiyeong0106\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /aiffel/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key = '0d59d7a17cbfbc47091fd5af0f49a1908e832c2e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87e0c61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define your validation dataset and dataloader\n",
    "# Assuming you have a validation_dataset and validation_dataloader\n",
    "\n",
    "def train():\n",
    "    # Default configuration for WandB\n",
    "    default_config = {\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"epoch\": 4,\n",
    "        \"batch_size\": 16\n",
    "    }\n",
    "\n",
    "    # Initialize WandB\n",
    "    wandb.init(config=default_config)\n",
    "    config = wandb.config\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config.epoch):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"],\n",
    "                \"attention_mask\": batch[\"attention_mask\"],\n",
    "                \"labels\": batch[\"label\"],\n",
    "            }\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 정확도 계산\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct_predictions += (predictions == batch[\"label\"]).sum().item()\n",
    "            total_samples += len(batch[\"label\"])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # Compute accuracy\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct_predictions += (predictions == batch[\"label\"]).sum().item()\n",
    "            total_samples += len(batch[\"label\"])\n",
    "\n",
    "            # Log every 10 batches\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                average_loss = total_loss / (batch_idx + 1)\n",
    "                accuracy = correct_predictions / total_samples\n",
    "\n",
    "                # Log metrics to WandB\n",
    "                wandb.log({\"Epoch\": epoch + 1, \"Batch\": batch_idx + 1, \"Training Loss\": average_loss, \"Training Accuracy\": accuracy})\n",
    "\n",
    "        # Log metrics for the entire epoch\n",
    "        average_loss = total_loss / len(train_dataloader)\n",
    "        accuracy = correct_predictions / total_samples\n",
    "        wandb.log({\"Epoch\": epoch + 1, \"Training Loss\": average_loss, \"Training Accuracy\": accuracy})\n",
    "        torch.save(model.state_dict(), f'PyModel{batch}.sd')\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_predictions, val_true_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for val_batch in test_dataloader:\n",
    "                val_inputs = {\n",
    "                    \"input_ids\": val_batch[\"input_ids\"],\n",
    "                    \"attention_mask\": val_batch[\"attention_mask\"],\n",
    "                    \"labels\": val_batch[\"label\"],\n",
    "                }\n",
    "                val_outputs = model(**val_inputs)\n",
    "                \n",
    "                # Get attention weights from the last layer (adjust as needed based on your model architecture)\n",
    "                attention_weights = val_outputs.attentions\n",
    "                \n",
    "                val_predictions.extend(torch.argmax(val_outputs.logits, dim=1).cpu().numpy())\n",
    "                val_true_labels.extend(val_batch[\"label\"].cpu().numpy())\n",
    "                \n",
    "                 # Visualize attention map for a random example in the batch\n",
    "                random_example_index = torch.randint(0, len(val_batch[\"input_ids\"]), (1,)).item()\n",
    "                random_attention_weights = attention_weights[random_example_index]\n",
    "\n",
    "                # Convert input_ids to tokens using the tokenizer\n",
    "                tokens = tokenizer.convert_ids_to_tokens(val_batch[\"input_ids\"][random_example_index].cpu().numpy())\n",
    "\n",
    "                # Visualize attention map\n",
    "                visualize_attention(val_batch[\"input_ids\"][random_example_index], random_attention_weights, tokens, val_batch[\"label\"][random_example_index].item())\n",
    "\n",
    "        val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
    "        wandb.log({\"Epoch\": epoch + 1, \"Validation Accuracy\": val_accuracy})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7002e362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1ee507f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 2j73xv61\n",
      "Sweep URL: https://wandb.ai/yunjiyeong0106/aiffel/sweeps/2j73xv61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mo8fdncr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.004736112158449822\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/aiffel/aiffel/Aiffel_online_Quest/DL/wandb/run-20240111_132529-mo8fdncr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yunjiyeong0106/aiffel/runs/mo8fdncr' target=\"_blank\">stellar-sweep-1</a></strong> to <a href='https://wandb.ai/yunjiyeong0106/aiffel' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/yunjiyeong0106/aiffel/sweeps/2j73xv61' target=\"_blank\">https://wandb.ai/yunjiyeong0106/aiffel/sweeps/2j73xv61</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yunjiyeong0106/aiffel' target=\"_blank\">https://wandb.ai/yunjiyeong0106/aiffel</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/yunjiyeong0106/aiffel/sweeps/2j73xv61' target=\"_blank\">https://wandb.ai/yunjiyeong0106/aiffel/sweeps/2j73xv61</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yunjiyeong0106/aiffel/runs/mo8fdncr' target=\"_blank\">https://wandb.ai/yunjiyeong0106/aiffel/runs/mo8fdncr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>Epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training Accuracy</td><td>▁▃▄▅▆▆▆▇▇▇▇▇▇███████</td></tr><tr><td>Training Loss</td><td>█▇▆▅▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch</td><td>190</td></tr><tr><td>Epoch</td><td>1</td></tr><tr><td>Training Accuracy</td><td>0.79715</td></tr><tr><td>Training Loss</td><td>0.54548</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-sweep-1</strong> at: <a href='https://wandb.ai/yunjiyeong0106/aiffel/runs/mo8fdncr' target=\"_blank\">https://wandb.ai/yunjiyeong0106/aiffel/runs/mo8fdncr</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240111_132529-mo8fdncr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run mo8fdncr errored: OSError(36, 'File name too long')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run mo8fdncr errored: OSError(36, 'File name too long')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "# Make sure to replace <<YOUR ENTITY NAME>> and <<YOUR PROJECT NAME>> with your Wandb entity and project names\n",
    "sweep_id = wandb.sweep(sweep_config,\n",
    "                       entity=\"yunjiyeong0106\",\n",
    "                       project=\"aiffel\")\n",
    "\n",
    "# Run the sweep\n",
    "wandb.agent(sweep_id, function=train, count=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2106162e",
   "metadata": {},
   "source": [
    " - warm-sweep-1 - 데이터 증강\n",
    " - peach-sweep-1 - kykim/bert-kor-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b85bf111",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'PyModel5.sd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9634deff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t_000</th>\n",
       "      <td>아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_001</th>\n",
       "      <td>우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_002</th>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_004</th>\n",
       "      <td>아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_005</th>\n",
       "      <td>그러니까 빨리 말해. 선생님 제발 살려주십시오.  비밀번호 틀릴 때마다 손톱 하나씩...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_495</th>\n",
       "      <td>미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_496</th>\n",
       "      <td>교수님 제 논문에 제 이름이 없나요?  아 무슨 논문말이야?  지난 번 냈던 논문이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_497</th>\n",
       "      <td>야 너  네 저요? 그래 너 왜요 돈좀 줘봐  돈 없어요 돈이 왜 없어 지갑은 폼이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_498</th>\n",
       "      <td>야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_499</th>\n",
       "      <td>엄마 저 그 돈 안해주시면 정말 큰일나요.  이유도 말하지 않고. 몇번째니 경민아....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "t_000  아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...\n",
       "t_001  우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...\n",
       "t_002  너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...\n",
       "t_004  아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...\n",
       "t_005  그러니까 빨리 말해. 선생님 제발 살려주십시오.  비밀번호 틀릴 때마다 손톱 하나씩...\n",
       "...                                                  ...\n",
       "t_495  미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...\n",
       "t_496  교수님 제 논문에 제 이름이 없나요?  아 무슨 논문말이야?  지난 번 냈던 논문이...\n",
       "t_497  야 너  네 저요? 그래 너 왜요 돈좀 줘봐  돈 없어요 돈이 왜 없어 지갑은 폼이...\n",
       "t_498  야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...\n",
       "t_499  엄마 저 그 돈 안해주시면 정말 큰일나요.  이유도 말하지 않고. 몇번째니 경민아....\n",
       "\n",
       "[400 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = os.getenv('HOME')+\"/aiffel/dktc/data/test.json\"\n",
    "# Create a list to store input texts\n",
    "df = pd.read_json(path).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85be2bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at kykim/bert-kor-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t_000</th>\n",
       "      <td>아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_001</th>\n",
       "      <td>우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_002</th>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_004</th>\n",
       "      <td>아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_005</th>\n",
       "      <td>그러니까 빨리 말해. 선생님 제발 살려주십시오.  비밀번호 틀릴 때마다 손톱 하나씩...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_495</th>\n",
       "      <td>미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_496</th>\n",
       "      <td>교수님 제 논문에 제 이름이 없나요?  아 무슨 논문말이야?  지난 번 냈던 논문이...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_497</th>\n",
       "      <td>야 너  네 저요? 그래 너 왜요 돈좀 줘봐  돈 없어요 돈이 왜 없어 지갑은 폼이...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_498</th>\n",
       "      <td>야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_499</th>\n",
       "      <td>엄마 저 그 돈 안해주시면 정말 큰일나요.  이유도 말하지 않고. 몇번째니 경민아....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  predicted_label\n",
       "t_000  아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...                1\n",
       "t_001  우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...                2\n",
       "t_002  너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...                2\n",
       "t_004  아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...                3\n",
       "t_005  그러니까 빨리 말해. 선생님 제발 살려주십시오.  비밀번호 틀릴 때마다 손톱 하나씩...                0\n",
       "...                                                  ...              ...\n",
       "t_495  미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...                2\n",
       "t_496  교수님 제 논문에 제 이름이 없나요?  아 무슨 논문말이야?  지난 번 냈던 논문이...                3\n",
       "t_497  야 너  네 저요? 그래 너 왜요 돈좀 줘봐  돈 없어요 돈이 왜 없어 지갑은 폼이...                1\n",
       "t_498  야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...                2\n",
       "t_499  엄마 저 그 돈 안해주시면 정말 큰일나요.  이유도 말하지 않고. 몇번째니 경민아....                0\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "\n",
    "# Load the saved model state dictionary\n",
    "model_state_dict = torch.load('PyModel5.sd')  # Change '123' to the appropriate batch number\n",
    "\n",
    "# Initialize the model architecture\n",
    "model = BertForSequenceClassification.from_pretrained(\"kykim/bert-kor-base\", num_labels=4)\n",
    "# Load the model state dictionary\n",
    "model.load_state_dict(model_state_dict)\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"kykim/bert-kor-base\")\n",
    "#model = BertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=4)\n",
    "\n",
    "# Assuming df is your DataFrame with a 'text' column\n",
    "texts = df['text']\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "test_dataset = CustomDataset(texts, tokenizer)\n",
    "\n",
    "# Create DataLoader instance\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "\n",
    "for test_batch in test_dataloader:\n",
    "    test_inputs = {\n",
    "        \"input_ids\": test_batch[\"input_ids\"],\n",
    "        \"attention_mask\": test_batch[\"attention_mask\"]\n",
    "    }\n",
    "    test_outputs = model(**test_inputs)\n",
    "    predictions.extend(torch.argmax(test_outputs.logits, dim=1).cpu().numpy())\n",
    "\n",
    "# Add predictions to the DataFrame\n",
    "df['predicted_label'] = predictions\n",
    "\n",
    "# Now, df contains the predicted labels in the 'predicted_label' column\n",
    "df[['text', 'predicted_label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e900f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df.drop('text',axis =1)\n",
    "df_result.rename(columns={'predicted_label':'class'}).reset_index().rename(columns={'index':'file_name'}).to_csv('predictions_kor.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa4e4dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_001</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_002</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_004</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_005</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>t_495</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>t_496</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>t_497</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>t_498</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>t_499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    file_name  class\n",
       "0       t_000      1\n",
       "1       t_001      2\n",
       "2       t_002      2\n",
       "3       t_004      3\n",
       "4       t_005      0\n",
       "..        ...    ...\n",
       "395     t_495      2\n",
       "396     t_496      3\n",
       "397     t_497      1\n",
       "398     t_498      2\n",
       "399     t_499      0\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('predictions_kor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cd768a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "class CustomDataset__(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=64):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataframe.iloc[idx]['text']\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39441ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'PyModel1_djstil.sd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2ef29a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load test data from test.json\n",
    "with open(os.getenv('HOME')+\"/aiffel/dktc//data/\"+\"test.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    test_data = json.load(json_file)\n",
    "\n",
    "# Create a list to store input texts\n",
    "test_texts = []\n",
    "\n",
    "# Extract text from test_data and store in the list\n",
    "for key, value in test_data.items():\n",
    "    test_texts.append(value[\"text\"])\n",
    "\n",
    "test_texts=pd.DataFrame(test_texts,columns=['text'])\n",
    "\n",
    "test_dataset = CustomDataset__(test_texts, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Dictionary to store predictions\n",
    "predictions_dict = {}\n",
    "\n",
    "# Evaluation loop for the test dataset\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, test_batch in enumerate(test_dataloader):\n",
    "        test_inputs = {\n",
    "            \"input_ids\": test_batch[\"input_ids\"],\n",
    "            \"attention_mask\": test_batch[\"attention_mask\"],\n",
    "        }\n",
    "        test_outputs = model(**test_inputs)\n",
    "        test_predictions = torch.argmax(test_outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "        # Format predictions\n",
    "        for i, prediction in enumerate(test_predictions):\n",
    "            key = f\"t_{idx * test_dataloader.batch_size + i:03d}\"\n",
    "            predictions_dict[key] = {\"class\": str(prediction + 1).zfill(2)}\n",
    "\n",
    "# Save predictions to a JSON file\n",
    "with open(\"predictions.json\", \"w\") as json_file:\n",
    "    json.dump(predictions_dict, json_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Predictions saved to predictions.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "32f639e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t_000': {'class': '04'},\n",
       " 't_001': {'class': '03'},\n",
       " 't_002': {'class': '03'},\n",
       " 't_003': {'class': '04'},\n",
       " 't_004': {'class': '04'},\n",
       " 't_005': {'class': '01'},\n",
       " 't_006': {'class': '04'},\n",
       " 't_007': {'class': '04'},\n",
       " 't_008': {'class': '01'},\n",
       " 't_009': {'class': '03'},\n",
       " 't_010': {'class': '03'},\n",
       " 't_011': {'class': '04'},\n",
       " 't_012': {'class': '01'},\n",
       " 't_013': {'class': '04'},\n",
       " 't_014': {'class': '01'},\n",
       " 't_015': {'class': '04'},\n",
       " 't_016': {'class': '04'},\n",
       " 't_017': {'class': '02'},\n",
       " 't_018': {'class': '02'},\n",
       " 't_019': {'class': '03'},\n",
       " 't_020': {'class': '03'},\n",
       " 't_021': {'class': '04'},\n",
       " 't_022': {'class': '04'},\n",
       " 't_023': {'class': '03'},\n",
       " 't_024': {'class': '02'},\n",
       " 't_025': {'class': '04'},\n",
       " 't_026': {'class': '02'},\n",
       " 't_027': {'class': '01'},\n",
       " 't_028': {'class': '01'},\n",
       " 't_029': {'class': '02'},\n",
       " 't_030': {'class': '02'},\n",
       " 't_031': {'class': '02'},\n",
       " 't_032': {'class': '02'},\n",
       " 't_033': {'class': '01'},\n",
       " 't_034': {'class': '03'},\n",
       " 't_035': {'class': '01'},\n",
       " 't_036': {'class': '02'},\n",
       " 't_037': {'class': '01'},\n",
       " 't_038': {'class': '03'},\n",
       " 't_039': {'class': '04'},\n",
       " 't_040': {'class': '02'},\n",
       " 't_041': {'class': '03'},\n",
       " 't_042': {'class': '03'},\n",
       " 't_043': {'class': '04'},\n",
       " 't_044': {'class': '02'},\n",
       " 't_045': {'class': '02'},\n",
       " 't_046': {'class': '02'},\n",
       " 't_047': {'class': '02'},\n",
       " 't_048': {'class': '04'},\n",
       " 't_049': {'class': '03'},\n",
       " 't_050': {'class': '03'},\n",
       " 't_051': {'class': '03'},\n",
       " 't_052': {'class': '01'},\n",
       " 't_053': {'class': '04'},\n",
       " 't_054': {'class': '03'},\n",
       " 't_055': {'class': '02'},\n",
       " 't_056': {'class': '03'},\n",
       " 't_057': {'class': '04'},\n",
       " 't_058': {'class': '03'},\n",
       " 't_059': {'class': '04'},\n",
       " 't_060': {'class': '04'},\n",
       " 't_061': {'class': '04'},\n",
       " 't_062': {'class': '03'},\n",
       " 't_063': {'class': '01'},\n",
       " 't_064': {'class': '01'},\n",
       " 't_065': {'class': '01'},\n",
       " 't_066': {'class': '03'},\n",
       " 't_067': {'class': '01'},\n",
       " 't_068': {'class': '03'},\n",
       " 't_069': {'class': '03'},\n",
       " 't_070': {'class': '03'},\n",
       " 't_071': {'class': '01'},\n",
       " 't_072': {'class': '04'},\n",
       " 't_073': {'class': '03'},\n",
       " 't_074': {'class': '04'},\n",
       " 't_075': {'class': '02'},\n",
       " 't_076': {'class': '03'},\n",
       " 't_077': {'class': '02'},\n",
       " 't_078': {'class': '03'},\n",
       " 't_079': {'class': '01'},\n",
       " 't_080': {'class': '03'},\n",
       " 't_081': {'class': '04'},\n",
       " 't_082': {'class': '04'},\n",
       " 't_083': {'class': '02'},\n",
       " 't_084': {'class': '04'},\n",
       " 't_085': {'class': '01'},\n",
       " 't_086': {'class': '03'},\n",
       " 't_087': {'class': '02'},\n",
       " 't_088': {'class': '02'},\n",
       " 't_089': {'class': '03'},\n",
       " 't_090': {'class': '03'},\n",
       " 't_091': {'class': '01'},\n",
       " 't_092': {'class': '03'},\n",
       " 't_093': {'class': '01'},\n",
       " 't_094': {'class': '02'},\n",
       " 't_095': {'class': '02'},\n",
       " 't_096': {'class': '02'},\n",
       " 't_097': {'class': '02'},\n",
       " 't_098': {'class': '03'},\n",
       " 't_099': {'class': '04'},\n",
       " 't_100': {'class': '04'},\n",
       " 't_101': {'class': '04'},\n",
       " 't_102': {'class': '04'},\n",
       " 't_103': {'class': '01'},\n",
       " 't_104': {'class': '02'},\n",
       " 't_105': {'class': '01'},\n",
       " 't_106': {'class': '01'},\n",
       " 't_107': {'class': '04'},\n",
       " 't_108': {'class': '04'},\n",
       " 't_109': {'class': '04'},\n",
       " 't_110': {'class': '02'},\n",
       " 't_111': {'class': '04'},\n",
       " 't_112': {'class': '03'},\n",
       " 't_113': {'class': '01'},\n",
       " 't_114': {'class': '03'},\n",
       " 't_115': {'class': '04'},\n",
       " 't_116': {'class': '02'},\n",
       " 't_117': {'class': '01'},\n",
       " 't_118': {'class': '01'},\n",
       " 't_119': {'class': '03'},\n",
       " 't_120': {'class': '01'},\n",
       " 't_121': {'class': '03'},\n",
       " 't_122': {'class': '04'},\n",
       " 't_123': {'class': '03'},\n",
       " 't_124': {'class': '04'},\n",
       " 't_125': {'class': '01'},\n",
       " 't_126': {'class': '04'},\n",
       " 't_127': {'class': '03'},\n",
       " 't_128': {'class': '04'},\n",
       " 't_129': {'class': '01'},\n",
       " 't_130': {'class': '03'},\n",
       " 't_131': {'class': '04'},\n",
       " 't_132': {'class': '01'},\n",
       " 't_133': {'class': '01'},\n",
       " 't_134': {'class': '03'},\n",
       " 't_135': {'class': '03'},\n",
       " 't_136': {'class': '03'},\n",
       " 't_137': {'class': '01'},\n",
       " 't_138': {'class': '04'},\n",
       " 't_139': {'class': '01'},\n",
       " 't_140': {'class': '03'},\n",
       " 't_141': {'class': '03'},\n",
       " 't_142': {'class': '01'},\n",
       " 't_143': {'class': '03'},\n",
       " 't_144': {'class': '01'},\n",
       " 't_145': {'class': '02'},\n",
       " 't_146': {'class': '04'},\n",
       " 't_147': {'class': '02'},\n",
       " 't_148': {'class': '02'},\n",
       " 't_149': {'class': '02'},\n",
       " 't_150': {'class': '02'},\n",
       " 't_151': {'class': '02'},\n",
       " 't_152': {'class': '04'},\n",
       " 't_153': {'class': '04'},\n",
       " 't_154': {'class': '03'},\n",
       " 't_155': {'class': '03'},\n",
       " 't_156': {'class': '02'},\n",
       " 't_157': {'class': '04'},\n",
       " 't_158': {'class': '03'},\n",
       " 't_159': {'class': '03'},\n",
       " 't_160': {'class': '04'},\n",
       " 't_161': {'class': '03'},\n",
       " 't_162': {'class': '03'},\n",
       " 't_163': {'class': '03'},\n",
       " 't_164': {'class': '02'},\n",
       " 't_165': {'class': '02'},\n",
       " 't_166': {'class': '04'},\n",
       " 't_167': {'class': '03'},\n",
       " 't_168': {'class': '04'},\n",
       " 't_169': {'class': '01'},\n",
       " 't_170': {'class': '01'},\n",
       " 't_171': {'class': '02'},\n",
       " 't_172': {'class': '01'},\n",
       " 't_173': {'class': '01'},\n",
       " 't_174': {'class': '03'},\n",
       " 't_175': {'class': '04'},\n",
       " 't_176': {'class': '04'},\n",
       " 't_177': {'class': '02'},\n",
       " 't_178': {'class': '03'},\n",
       " 't_179': {'class': '03'},\n",
       " 't_180': {'class': '04'},\n",
       " 't_181': {'class': '03'},\n",
       " 't_182': {'class': '02'},\n",
       " 't_183': {'class': '04'},\n",
       " 't_184': {'class': '04'},\n",
       " 't_185': {'class': '02'},\n",
       " 't_186': {'class': '03'},\n",
       " 't_187': {'class': '04'},\n",
       " 't_188': {'class': '01'},\n",
       " 't_189': {'class': '04'},\n",
       " 't_190': {'class': '04'},\n",
       " 't_191': {'class': '03'},\n",
       " 't_192': {'class': '02'},\n",
       " 't_193': {'class': '03'},\n",
       " 't_194': {'class': '03'},\n",
       " 't_195': {'class': '03'},\n",
       " 't_196': {'class': '01'},\n",
       " 't_197': {'class': '01'},\n",
       " 't_198': {'class': '01'},\n",
       " 't_199': {'class': '02'},\n",
       " 't_200': {'class': '01'},\n",
       " 't_201': {'class': '04'},\n",
       " 't_202': {'class': '02'},\n",
       " 't_203': {'class': '02'},\n",
       " 't_204': {'class': '03'},\n",
       " 't_205': {'class': '02'},\n",
       " 't_206': {'class': '04'},\n",
       " 't_207': {'class': '04'},\n",
       " 't_208': {'class': '04'},\n",
       " 't_209': {'class': '04'},\n",
       " 't_210': {'class': '02'},\n",
       " 't_211': {'class': '04'},\n",
       " 't_212': {'class': '04'},\n",
       " 't_213': {'class': '01'},\n",
       " 't_214': {'class': '03'},\n",
       " 't_215': {'class': '04'},\n",
       " 't_216': {'class': '03'},\n",
       " 't_217': {'class': '03'},\n",
       " 't_218': {'class': '02'},\n",
       " 't_219': {'class': '04'},\n",
       " 't_220': {'class': '01'},\n",
       " 't_221': {'class': '03'},\n",
       " 't_222': {'class': '03'},\n",
       " 't_223': {'class': '02'},\n",
       " 't_224': {'class': '03'},\n",
       " 't_225': {'class': '04'},\n",
       " 't_226': {'class': '03'},\n",
       " 't_227': {'class': '04'},\n",
       " 't_228': {'class': '02'},\n",
       " 't_229': {'class': '03'},\n",
       " 't_230': {'class': '04'},\n",
       " 't_231': {'class': '03'},\n",
       " 't_232': {'class': '01'},\n",
       " 't_233': {'class': '04'},\n",
       " 't_234': {'class': '01'},\n",
       " 't_235': {'class': '01'},\n",
       " 't_236': {'class': '01'},\n",
       " 't_237': {'class': '03'},\n",
       " 't_238': {'class': '04'},\n",
       " 't_239': {'class': '02'},\n",
       " 't_240': {'class': '03'},\n",
       " 't_241': {'class': '04'},\n",
       " 't_242': {'class': '04'},\n",
       " 't_243': {'class': '03'},\n",
       " 't_244': {'class': '04'},\n",
       " 't_245': {'class': '02'},\n",
       " 't_246': {'class': '01'},\n",
       " 't_247': {'class': '03'},\n",
       " 't_248': {'class': '02'},\n",
       " 't_249': {'class': '02'},\n",
       " 't_250': {'class': '04'},\n",
       " 't_251': {'class': '04'},\n",
       " 't_252': {'class': '01'},\n",
       " 't_253': {'class': '03'},\n",
       " 't_254': {'class': '01'},\n",
       " 't_255': {'class': '04'},\n",
       " 't_256': {'class': '02'},\n",
       " 't_257': {'class': '02'},\n",
       " 't_258': {'class': '02'},\n",
       " 't_259': {'class': '02'},\n",
       " 't_260': {'class': '03'},\n",
       " 't_261': {'class': '04'},\n",
       " 't_262': {'class': '04'},\n",
       " 't_263': {'class': '01'},\n",
       " 't_264': {'class': '01'},\n",
       " 't_265': {'class': '04'},\n",
       " 't_266': {'class': '02'},\n",
       " 't_267': {'class': '02'},\n",
       " 't_268': {'class': '03'},\n",
       " 't_269': {'class': '04'},\n",
       " 't_270': {'class': '02'},\n",
       " 't_271': {'class': '03'},\n",
       " 't_272': {'class': '04'},\n",
       " 't_273': {'class': '01'},\n",
       " 't_274': {'class': '04'},\n",
       " 't_275': {'class': '03'},\n",
       " 't_276': {'class': '02'},\n",
       " 't_277': {'class': '02'},\n",
       " 't_278': {'class': '01'},\n",
       " 't_279': {'class': '03'},\n",
       " 't_280': {'class': '04'},\n",
       " 't_281': {'class': '02'},\n",
       " 't_282': {'class': '02'},\n",
       " 't_283': {'class': '03'},\n",
       " 't_284': {'class': '04'},\n",
       " 't_285': {'class': '04'},\n",
       " 't_286': {'class': '02'},\n",
       " 't_287': {'class': '04'},\n",
       " 't_288': {'class': '03'},\n",
       " 't_289': {'class': '01'},\n",
       " 't_290': {'class': '04'},\n",
       " 't_291': {'class': '03'},\n",
       " 't_292': {'class': '02'},\n",
       " 't_293': {'class': '03'},\n",
       " 't_294': {'class': '02'},\n",
       " 't_295': {'class': '02'},\n",
       " 't_296': {'class': '04'},\n",
       " 't_297': {'class': '02'},\n",
       " 't_298': {'class': '01'},\n",
       " 't_299': {'class': '03'},\n",
       " 't_300': {'class': '03'},\n",
       " 't_301': {'class': '01'},\n",
       " 't_302': {'class': '02'},\n",
       " 't_303': {'class': '04'},\n",
       " 't_304': {'class': '02'},\n",
       " 't_305': {'class': '02'},\n",
       " 't_306': {'class': '03'},\n",
       " 't_307': {'class': '01'},\n",
       " 't_308': {'class': '03'},\n",
       " 't_309': {'class': '02'},\n",
       " 't_310': {'class': '04'},\n",
       " 't_311': {'class': '04'},\n",
       " 't_312': {'class': '01'},\n",
       " 't_313': {'class': '01'},\n",
       " 't_314': {'class': '01'},\n",
       " 't_315': {'class': '02'},\n",
       " 't_316': {'class': '01'},\n",
       " 't_317': {'class': '03'},\n",
       " 't_318': {'class': '03'},\n",
       " 't_319': {'class': '02'},\n",
       " 't_320': {'class': '01'},\n",
       " 't_321': {'class': '01'},\n",
       " 't_322': {'class': '02'},\n",
       " 't_323': {'class': '04'},\n",
       " 't_324': {'class': '04'},\n",
       " 't_325': {'class': '04'},\n",
       " 't_326': {'class': '03'},\n",
       " 't_327': {'class': '03'},\n",
       " 't_328': {'class': '04'},\n",
       " 't_329': {'class': '02'},\n",
       " 't_330': {'class': '03'},\n",
       " 't_331': {'class': '01'},\n",
       " 't_332': {'class': '01'},\n",
       " 't_333': {'class': '01'},\n",
       " 't_334': {'class': '01'},\n",
       " 't_335': {'class': '03'},\n",
       " 't_336': {'class': '02'},\n",
       " 't_337': {'class': '02'},\n",
       " 't_338': {'class': '01'},\n",
       " 't_339': {'class': '04'},\n",
       " 't_340': {'class': '01'},\n",
       " 't_341': {'class': '02'},\n",
       " 't_342': {'class': '04'},\n",
       " 't_343': {'class': '04'},\n",
       " 't_344': {'class': '02'},\n",
       " 't_345': {'class': '04'},\n",
       " 't_346': {'class': '03'},\n",
       " 't_347': {'class': '01'},\n",
       " 't_348': {'class': '03'},\n",
       " 't_349': {'class': '02'},\n",
       " 't_350': {'class': '01'},\n",
       " 't_351': {'class': '01'},\n",
       " 't_352': {'class': '03'},\n",
       " 't_353': {'class': '04'},\n",
       " 't_354': {'class': '01'},\n",
       " 't_355': {'class': '03'},\n",
       " 't_356': {'class': '03'},\n",
       " 't_357': {'class': '01'},\n",
       " 't_358': {'class': '04'},\n",
       " 't_359': {'class': '03'},\n",
       " 't_360': {'class': '03'},\n",
       " 't_361': {'class': '04'},\n",
       " 't_362': {'class': '01'},\n",
       " 't_363': {'class': '03'},\n",
       " 't_364': {'class': '04'},\n",
       " 't_365': {'class': '02'},\n",
       " 't_366': {'class': '03'},\n",
       " 't_367': {'class': '03'},\n",
       " 't_368': {'class': '02'},\n",
       " 't_369': {'class': '03'},\n",
       " 't_370': {'class': '02'},\n",
       " 't_371': {'class': '03'},\n",
       " 't_372': {'class': '04'},\n",
       " 't_373': {'class': '03'},\n",
       " 't_374': {'class': '04'},\n",
       " 't_375': {'class': '02'},\n",
       " 't_376': {'class': '03'},\n",
       " 't_377': {'class': '01'},\n",
       " 't_378': {'class': '03'},\n",
       " 't_379': {'class': '04'},\n",
       " 't_380': {'class': '03'},\n",
       " 't_381': {'class': '03'},\n",
       " 't_382': {'class': '03'},\n",
       " 't_383': {'class': '04'},\n",
       " 't_384': {'class': '03'},\n",
       " 't_385': {'class': '04'},\n",
       " 't_386': {'class': '02'},\n",
       " 't_387': {'class': '03'},\n",
       " 't_388': {'class': '04'},\n",
       " 't_389': {'class': '04'},\n",
       " 't_390': {'class': '04'},\n",
       " 't_391': {'class': '04'},\n",
       " 't_392': {'class': '01'},\n",
       " 't_393': {'class': '02'},\n",
       " 't_394': {'class': '02'},\n",
       " 't_395': {'class': '03'},\n",
       " 't_396': {'class': '02'},\n",
       " 't_397': {'class': '02'},\n",
       " 't_398': {'class': '01'},\n",
       " 't_399': {'class': '02'}}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the test data from the JSON file\n",
    "with open('predictions.json', 'r') as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "test_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
